#!/usr/bin/env python3
"""
ğŸ”¬ ëŒ€í™” ìš”ì•½ ì•™ìƒë¸” ì¶”ë¡  ì‹œìŠ¤í…œ (ê°„ì†Œí™”ëœ í•¨ìˆ˜ ê¸°ë°˜ ë²„ì „)

5ê°€ì§€ ì•™ìƒë¸” ê¸°ë²•ì„ ë‹¨ìˆœí•œ í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ê³ , 
ê° ê¸°ë²• ì™„ë£Œ ì‹œ ì¦‰ì‹œ ROUGE ì ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

ì•™ìƒë¸” ë°©ì‹:
1. hard_voting   - ê° ëª¨ë¸ì´ ì™„ì „í•œ í…ìŠ¤íŠ¸ ìƒì„± í›„ í† í°ë³„ ë‹¤ìˆ˜ê²°
2. soft_voting   - ê° ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìµœì  í›„ë³´ ì„ íƒ
3. length_based  - ê° ëª¨ë¸ ê²°ê³¼ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ì„ íƒ
4. logit_level   - ìµœì í™”ëœ Logit ì•™ìƒë¸” (Nucleus Sampling + Beam Search)
5. realtime_token- ë§¤ í† í°ë§ˆë‹¤ ëª¨ë“  ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìƒì„±

ì‚¬ìš©ë²•:
- python ensemble_inference_simple.py --mode=all           # ëª¨ë“  ë°©ì‹ ë¹„êµ
- python ensemble_inference_simple.py --mode=hard_voting   # í•˜ë“œ ë³´íŒ…ë§Œ
- python ensemble_inference_simple.py --mode=soft_voting   # ì†Œí”„íŠ¸ ë³´íŒ…ë§Œ
- python ensemble_inference_simple.py --mode=length_based  # ê¸¸ì´ ê¸°ë°˜ë§Œ
- python ensemble_inference_simple.py --mode=realtime_token # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”ë§Œ
- python ensemble_inference_simple.py --mode=logit_level    # ìµœì í™”ëœ Logit ì•™ìƒë¸”ë§Œ
"""

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append('../utils')
import log_util as log
from baseline import compute_metrics

# ê²€ì¦ ë°ì´í„° ê°œìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)
DEV_DATA_LIMIT = None #50  # 0ì´ë‚˜ Noneì´ ì•„ë‹Œ ì •ìˆ˜ë¥¼ ì„¤ì •í•˜ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)
TEST_DATA_LIMIT = None #50  # 0ì´ë‚˜ Noneì´ ì•„ë‹Œ ì •ìˆ˜ë¥¼ ì„¤ì •í•˜ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©

import argparse
import json
import time
import zipfile
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import random

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from tqdm import tqdm
from transformers import AutoTokenizer, BartForConditionalGeneration
from rouge import Rouge
# baseline.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ ì„í¬íŠ¸
from baseline import Preprocess, DatasetForVal, compute_metrics
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import tempfile

# ì‹œë“œ ì„¤ì •
def set_seed(seed: int = 42):
    """ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    # CUDNN ì¬í˜„ì„± ì„¤ì • (ensemble_inference_best.pyì™€ ë™ì¼)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# =============================================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_model_paths() -> List[str]:
    """ensemble_inference.pyì—ì„œ ì‚¬ìš©í–ˆë˜ ì •í™•íˆ ë™ì¼í•œ 3ê°œ ëª¨ë¸ ì‚¬ìš©"""
    # ensemble_inference.pyì—ì„œ ì§€ì •í–ˆë˜ ì •í™•í•œ ëª¨ë¸ë“¤
    model_paths = [
        "./models/model_baseline_20250805_070447.zip",  
        "./models/model_baseline_20250805_060913.zip",
        "./models/model_baseline_20250805_094805.zip",
    ]
    
    # ì¡´ì¬í•˜ëŠ” ëª¨ë¸ íŒŒì¼ë§Œ í•„í„°ë§
    existing_model_paths = []
    for model_path in model_paths:
        if os.path.exists(model_path):
            existing_model_paths.append(model_path)
            log.info(f"ëª¨ë¸ íŒŒì¼ í™•ì¸: {model_path}")
        else:
            log.warning(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {model_path}")
    
    if not existing_model_paths:
        log.error("ì§€ì •ëœ ëª¨ë¸ íŒŒì¼ë“¤ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸” ì§„í–‰")
    return existing_model_paths

def load_models(model_paths: List[str], device: str) -> Tuple[List, List, List, List]:
    """
    ëª¨ë¸ë“¤ì„ ë¡œë“œí•˜ê³  ë°˜í™˜ (ì›ë˜ ensemble_inference.pyì˜ ë¡œì§ ì‚¬ìš©)
    
    Returns:
        Tuple[models, tokenizers, configs, metadata_list]
    """
    import yaml
    import shutil
    
    models = []
    tokenizers = []
    configs = []
    metadata_list = []
    
    log.info(f"ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {len(model_paths)}ê°œ ëª¨ë¸")
    log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    log.info("ëª¨ë¸ë“¤ ë¡œë”© ì‹œì‘...")
    
    for i, model_path in enumerate(model_paths):
        log.info(f"ëª¨ë¸ {i+1}/{len(model_paths)} ë¡œë”© ì¤‘: {model_path}")
        
        temp_dir = f"temp_load_{int(time.time())}"
        
        try:
            # ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë”©
            log.info(f"ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë”© ì‹œì‘: {model_path}")
            with zipfile.ZipFile(model_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # ì„¤ì • ë¡œë“œ
            config_path = os.path.join(temp_dir, "config.yaml")
            with open(config_path, "r", encoding='utf-8') as f:
                config = yaml.safe_load(f)
            log.info("ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
                
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = os.path.join(temp_dir, "metadata.json")
            with open(metadata_path, "r", encoding='utf-8') as f:
                metadata = json.load(f)
            log.info("ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
            
            # ì‹œë“œ ì„¤ì • (configì—ì„œ - ensemble_inference_best.pyì™€ ë™ì¼)
            if 'training' in config and 'seed' in config['training']:
                seed = config['training']['seed']
                set_seed(seed)
                log.info(f"ëª¨ë¸ ë¡œë”© ì‹œ ì‹œë“œ ì„¤ì •: {seed}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ ë¨¼ì € (ì‹¤ì œ vocab í¬ê¸° í™•ì¸ìš©)
            tokenizer_dir = os.path.join(temp_dir, "tokenizer")
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
            
            # Special tokens ì¶”ê°€ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
            if 'tokenizer' in config and 'special_tokens' in config['tokenizer']:
                special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}
                tokenizer.add_special_tokens(special_tokens_dict)
                log.info(f"Special tokens ì¶”ê°€: {config['tokenizer']['special_tokens']}")
            
            log.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë¸ ë¡œë“œ (temp_dir ë£¨íŠ¸ì— ëª¨ë¸ íŒŒì¼ë“¤ì´ ìˆìŒ)
            from transformers import BartConfig
            
            # ì €ì¥ëœ ëª¨ë¸ì˜ config.json ì§ì ‘ ë¡œë“œ ë° ìˆ˜ì •
            model_name = metadata.get('model_info', {}).get('model_name', 'digit82/kobart-summarization')
            bart_config = BartConfig.from_pretrained(model_name)
            actual_vocab_size = len(tokenizer)
            bart_config.vocab_size = actual_vocab_size
            
            model = BartForConditionalGeneration.from_pretrained(temp_dir, config=bart_config)
            
            # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ vocab í¬ê¸° ë§ì¶”ê¸°
            model.resize_token_embeddings(len(tokenizer))
            model.to(device)
            model.eval()
            
            log.info(f"BART ì„¤ì • ë¡œë“œ ì™„ë£Œ, vocab_size: {len(tokenizer)}")
            log.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            models.append(model)
            tokenizers.append(tokenizer)
            configs.append(config)
            metadata_list.append(metadata)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            shutil.rmtree(temp_dir)
            
            log.info(f"ëª¨ë¸ {i+1} ë¡œë”© ì™„ë£Œ: {metadata.get('wandb_run_name', 'unknown')} (device: {device})")
            
        except Exception as e:
            log.error(f"ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
            continue
    
    log.info(f"ì´ {len(models)}ê°œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    return models, tokenizers, configs, metadata_list

def calculate_rouge_scores(predictions: List[str], references: List[str], method_name: str, remove_tokens: List[str]) -> Dict[str, float]:
    """
    ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ì¦‰ì‹œ ì¶œë ¥
    
    Args:
        predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        references: ì°¸ì¡° ë‹µì•ˆ ë¦¬ìŠ¤íŠ¸  
        method_name: ì•™ìƒë¸” ë°©ë²• ì´ë¦„
        remove_tokens: ì œê±°í•  í† í°ë“¤
        
    Returns:
        ROUGE ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    rouge = Rouge()
    
    # í† í° ì œê±° ë° ì •ê·œí™”
    cleaned_predictions = []
    cleaned_references = []
    
    for pred, ref in zip(predictions, references):
        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        pred_clean = pred
        ref_clean = ref
        for token in remove_tokens:
            pred_clean = pred_clean.replace(token, " ")
            ref_clean = ref_clean.replace(token, " ")
        
        # ê³µë°± ì •ë¦¬
        pred_clean = " ".join(pred_clean.split()).strip()
        ref_clean = " ".join(ref_clean.split()).strip()
        
        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        if not pred_clean.strip():
            pred_clean = "empty"
        if not ref_clean.strip():
            ref_clean = "empty"
            
        cleaned_predictions.append(pred_clean)
        cleaned_references.append(ref_clean)
    
    # ROUGE ê³„ì‚°
    try:
        rouge_results = rouge.get_scores(cleaned_predictions, cleaned_references, avg=True)
        rouge_scores = {key: value["f"] for key, value in rouge_results.items()}
        rouge_scores['rouge-avg'] = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
        
        # ì¦‰ì‹œ ê²°ê³¼ ì¶œë ¥
        log.info(f"")
        log.info(f"ğŸ¯ {method_name} ì™„ë£Œ!")
        log.info(f"ğŸ“Š ROUGE ì ìˆ˜:")
        log.info(f"   - ROUGE-1: {rouge_scores['rouge-1']:.6f}")
        log.info(f"   - ROUGE-2: {rouge_scores['rouge-2']:.6f}")  
        log.info(f"   - ROUGE-L: {rouge_scores['rouge-l']:.6f}")
        log.info(f"   - ROUGE-avg: {rouge_scores['rouge-avg']:.6f}")
        log.info(f"")
        
        return rouge_scores
        
    except Exception as e:
        log.error(f"ROUGE ê³„ì‚° ì‹¤íŒ¨ ({method_name}): {e}")
        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'rouge-avg': 0.0}

def save_results_to_json(results: Dict[str, Any], timestamp: str) -> str:
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    results_dir = "./ensemble_results"
    os.makedirs(results_dir, exist_ok=True)
    
    json_path = os.path.join(results_dir, f"comprehensive_experiment_{timestamp}.json")
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    log.info(f"ğŸ“ ì‹¤í—˜ ê²°ê³¼ ì €ì¥: {json_path}")
    return json_path

def calculate_rouge_scores_with_baseline(predictions_ids: List[List[int]], labels_ids: List[List[int]], 
                                         tokenizer, config, method_name: str) -> Dict[str, float]:
    """
    baseline.pyì˜ compute_metricsë¥¼ ì‚¬ìš©í•˜ì—¬ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ì¦‰ì‹œ ì¶œë ¥
    
    Args:
        predictions_ids: ì˜ˆì¸¡ ê²°ê³¼ í† í° ID ë¦¬ìŠ¤íŠ¸
        labels_ids: ì°¸ì¡° ë‹µì•ˆ í† í° ID ë¦¬ìŠ¤íŠ¸
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        method_name: ì•™ìƒë¸” ë°©ë²• ì´ë¦„
        
    Returns:
        ROUGE ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    try:
        # íŒ¨ë”©ì„ ì‚¬ìš©í•´ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶”ê¸° (ìˆ˜ë™ íŒ¨ë”© êµ¬í˜„)
        
        # ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
        max_pred_len = max(len(seq) for seq in predictions_ids) if predictions_ids else 1
        max_label_len = max(len(seq) for seq in labels_ids) if labels_ids else 1
        max_len = max(max_pred_len, max_label_len)
        
        # íŒ¨ë”© ì ìš©
        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
        
        padded_predictions = []
        padded_labels = []
        
        for seq in predictions_ids:
            if len(seq) < max_len:
                padded_seq = seq + [pad_token_id] * (max_len - len(seq))
            else:
                padded_seq = seq[:max_len]
            padded_predictions.append(padded_seq)
        
        for seq in labels_ids:
            if len(seq) < max_len:
                # labelsì—ëŠ” -100ìœ¼ë¡œ íŒ¨ë”© (compute_metricsê°€ ë¬´ì‹œí•¨)
                padded_seq = seq + [-100] * (max_len - len(seq))
            else:
                padded_seq = seq[:max_len]
            padded_labels.append(padded_seq)
        
        # baseline.py í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„
        predictions_array = np.array(padded_predictions)
        labels_array = np.array(padded_labels)
        
        # ì˜ˆì¸¡ ê°ì²´ ìƒì„± (baseline.pyì˜ compute_metricsì™€ í˜¸í™˜)
        pred_object = type('PredictionOutput', (), {
            'predictions': predictions_array,
            'label_ids': labels_array
        })()
        
        # ë””ë²„ê¹…: compute_metrics í˜¸ì¶œ ì „ ë°ì´í„° í™•ì¸
        log.info(f"ğŸ” compute_metrics í˜¸ì¶œ - predictions shape: {predictions_array.shape}, labels shape: {labels_array.shape}")
        log.info(f"ğŸ” ì˜ˆì¸¡ ìƒ˜í”Œ: {predictions_array[0][:10]} ...")
        log.info(f"ğŸ” ë¼ë²¨ ìƒ˜í”Œ: {labels_array[0][:10]} ...")
        
        # baseline.pyì˜ compute_metrics ì‚¬ìš©
        metrics = compute_metrics(config, tokenizer, pred_object)
        
        # ë””ë²„ê¹…: compute_metrics ê²°ê³¼ í™•ì¸
        log.info(f"ğŸ” compute_metrics ì›ë³¸ ê²°ê³¼: {metrics}")
        
        # ê²°ê³¼ë¥¼ ì•™ìƒë¸” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        rouge_scores = {
            'rouge-1': metrics.get('rouge-1', 0.0),
            'rouge-2': metrics.get('rouge-2', 0.0),
            'rouge-l': metrics.get('rouge-l', 0.0)
        }
        rouge_scores['rouge-avg'] = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
        
        # ì¦‰ì‹œ ê²°ê³¼ ì¶œë ¥
        log.info(f"")
        log.info(f"ğŸ¯ {method_name} ì™„ë£Œ!")
        log.info(f"ğŸ“Š ROUGE ì ìˆ˜:")
        log.info(f"   - ROUGE-1: {rouge_scores['rouge-1']:.6f}")
        log.info(f"   - ROUGE-2: {rouge_scores['rouge-2']:.6f}")
        log.info(f"   - ROUGE-L: {rouge_scores['rouge-l']:.6f}")
        log.info(f"   - ROUGE-avg: {rouge_scores['rouge-avg']:.6f}")
        log.info(f"")
        
        return rouge_scores
        
    except Exception as e:
        log.error(f"ROUGE ê³„ì‚° ì‹¤íŒ¨ ({method_name}): {e}")
        import traceback
        log.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'rouge-avg': 0.0}

def convert_text_predictions_to_baseline_format(predictions: List[str], reference_summaries: List[str], 
                                                tokenizer, config, method_name: str) -> Dict[str, float]:
    """
    í…ìŠ¤íŠ¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ baseline.py ë°©ì‹ìœ¼ë¡œ ROUGE ê³„ì‚°í•˜ëŠ” ê³µí†µ í•¨ìˆ˜
    
    Args:
        predictions: ì˜ˆì¸¡ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        reference_summaries: ì°¸ì¡° í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        method_name: ì•™ìƒë¸” ë°©ë²• ì´ë¦„
        
    Returns:
        ROUGE ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    predictions_ids = []
    labels_ids = []
    
    # ì°¸ì¡° ë‹µì•ˆì„ í† í° IDë¡œ ë³€í™˜
    for ref_text in reference_summaries:
        ref_tokens = tokenizer(ref_text, return_tensors="pt", truncation=True, padding=False, max_length=512)
        labels_ids.append(ref_tokens['input_ids'][0].tolist())
    
    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í† í° IDë¡œ ë³€í™˜
    for pred_text in predictions:
        pred_tokens = tokenizer(pred_text, return_tensors="pt", truncation=True, padding=False, max_length=512)
        predictions_ids.append(pred_tokens['input_ids'][0].tolist())
    
    # baseline.pyì˜ compute_metrics ì‚¬ìš©í•˜ì—¬ ROUGE ê³„ì‚°
    return calculate_rouge_scores_with_baseline(predictions_ids, labels_ids, tokenizer, config, method_name)

def run_test_inference_for_method(models: List, tokenizers: List, configs: List, method_name: str, test_data: pd.DataFrame, timestamp: str) -> str:
    """
    íŠ¹ì • ì•™ìƒë¸” ë°©ë²•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ìˆ˜í–‰ ë° CSV ì €ì¥
    
    Args:
        models: ë¡œë”©ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        tokenizers: í† í¬ë‚˜ì´ì € ë¦¬ìŠ¤íŠ¸  
        configs: ì„¤ì • ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        method_name: ì•™ìƒë¸” ë°©ë²• ì´ë¦„
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°í”„ë ˆì„
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        
    Returns:
        str: ì €ì¥ëœ CSV íŒŒì¼ ê²½ë¡œ
    """
    log.info(f"ğŸ¯ {method_name} í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì‹œì‘...")
    
    input_texts = test_data['dialogue'].tolist()
    test_ids = test_data['fname'].tolist()
    predictions = []
    remove_tokens = configs[0]['inference']['remove_tokens']
    
    # ë°©ë²•ë³„ ì¶”ë¡  ìˆ˜í–‰
    if method_name == "hard_voting":
        predictions = test_inference_hard_voting(models, tokenizers, configs, input_texts, remove_tokens)
    elif method_name == "soft_voting":
        predictions = test_inference_soft_voting(models, tokenizers, configs, input_texts, remove_tokens)
    elif method_name == "length_based":
        predictions = test_inference_length_based(models, tokenizers, configs, input_texts, remove_tokens)
    elif method_name == "logit_level":
        predictions = test_inference_logit_level(models, tokenizers, configs, input_texts, remove_tokens)
    elif method_name == "realtime_token_ensemble":
        predictions = test_inference_realtime_token(models, tokenizers, configs, input_texts, remove_tokens)
    else:
        log.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°©ë²•: {method_name}")
        return None
    
    # CSV ì €ì¥
    results_dir = "./prediction"
    os.makedirs(results_dir, exist_ok=True)
    
    csv_path = os.path.join(results_dir, f"{method_name}_{timestamp}.csv")
    result_df = pd.DataFrame({
        'id': test_ids,
        'summary': predictions
    })
    
    result_df.to_csv(csv_path, index=False, encoding='utf-8')
    log.info(f"ğŸ“ {method_name} í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {csv_path}")
    
    return csv_path

# =============================================================================
# ê° ì•™ìƒë¸” ê¸°ë²•ë³„ í‰ê°€ í•¨ìˆ˜ë“¤
# =============================================================================

def prepare_validation_dataset_for_ensemble(config, preprocessor, tokenizer):
    """
    baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        preprocessor: ë°ì´í„° ì „ì²˜ë¦¬ê¸°
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        DatasetForVal: ê²€ì¦ ë°ì´í„°ì…‹
    """
    log.info("ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘ (baseline.py ë°©ì‹)")
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    data_path = config['general']['data_path']
    val_file_path = os.path.join(data_path, 'dev.csv')
    val_data = preprocessor.make_set_as_df(val_file_path)
    
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    
    # í† í¬ë‚˜ì´ì € ì ìš© (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    val_tokenized_encoder_inputs = tokenizer(
        encoder_input_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['encoder_max_len'], 
        return_token_type_ids=False
    )
    
    val_tokenized_decoder_inputs = tokenizer(
        decoder_input_val, 
        return_tensors="pt", 
        padding=True, 
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['decoder_max_len'], 
        return_token_type_ids=False
    )
    
    val_tokenized_decoder_outputs = tokenizer(
        decoder_output_val, 
        return_tensors="pt", 
        padding=True, 
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['decoder_max_len'], 
        return_token_type_ids=False
    )
    
    # ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„± (baseline.pyì™€ ë™ì¼)
    val_inputs_dataset = DatasetForVal(
        val_tokenized_encoder_inputs, 
        val_tokenized_decoder_inputs, 
        val_tokenized_decoder_outputs,
        len(encoder_input_val)
    )
    
    log.info("ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    return val_inputs_dataset

def evaluate_single_model_with_baseline(model, tokenizer, config):
    """
    baseline.py ë°©ì‹ìœ¼ë¡œ ë‹¨ì¼ ëª¨ë¸ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
    
    Args:
        model: ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì •
        
    Returns:
        dict: ROUGE ë©”íŠ¸ë¦­ ê²°ê³¼
    """
    log.info("baseline.py ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
    
    # ë°ì´í„° ì „ì²˜ë¦¬ê¸° ìƒì„± (baseline.pyì™€ ë™ì¼)
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])
    
    # ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    val_inputs_dataset = prepare_validation_dataset_for_ensemble(config, preprocessor, tokenizer)
    
    # Seq2SeqTrainingArguments ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°, wandb ë¹„í™œì„±í™”)
    training_args = Seq2SeqTrainingArguments(
        output_dir='./temp_eval_results',
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        predict_with_generate=True,
        generation_max_length=config['inference']['generate_max_length'],
        generation_num_beams=config['inference']['num_beams'],
        include_inputs_for_metrics=False,
        report_to=[],  # wandb ë¹„í™œì„±í™”
        logging_strategy="no"
    )
    
    # Seq2SeqTrainer ìƒì„± (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        tokenizer=tokenizer,
        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred)
    )
    
    log.info("Seq2SeqTrainerë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹œì‘")
    # í‰ê°€ ìˆ˜í–‰ (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    eval_results = trainer.evaluate(eval_dataset=val_inputs_dataset)
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    import shutil
    if os.path.exists('./temp_eval_results'):
        shutil.rmtree('./temp_eval_results')
    
    log.info("baseline.py ë°©ì‹ ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ")
    
    # rouge ê²°ê³¼ë§Œ ì¶”ì¶œ
    rouge_results = {
        'rouge-1': eval_results['eval_rouge-1'],
        'rouge-2': eval_results['eval_rouge-2'], 
        'rouge-l': eval_results['eval_rouge-l'],
        'rouge-avg': (eval_results['eval_rouge-1'] + eval_results['eval_rouge-2'] + eval_results['eval_rouge-l']) / 3
    }
    
    return rouge_results

def evaluate_individual_models(models: List, tokenizers: List, configs: List, metadata_list: List, val_data: pd.DataFrame) -> List[Dict]:
    """ê°œë³„ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ í‰ê°€ (baseline.py ë°©ì‹ ì‚¬ìš©)"""
    log.info("ğŸ” ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    
    individual_scores = []
    
    for i, (model, tokenizer, config, metadata) in enumerate(zip(models, tokenizers, configs, metadata_list)):
        log.info(f"ëª¨ë¸ {i+1}/{len(models)} í‰ê°€ ì¤‘...")
        
        # baseline.py ë°©ì‹ìœ¼ë¡œ ì •í™•í•œ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
        rouge_scores = evaluate_single_model_with_baseline(model, tokenizer, config)
        
        # ROUGE ì ìˆ˜ ì¦‰ì‹œ ì¶œë ¥
        log.info(f"")
        log.info(f"ğŸ¯ ê°œë³„ ëª¨ë¸ {i+1} ì™„ë£Œ!")
        log.info(f"ğŸ“Š ROUGE ì ìˆ˜:")
        log.info(f"   - ROUGE-1: {rouge_scores['rouge-1']:.6f}")
        log.info(f"   - ROUGE-2: {rouge_scores['rouge-2']:.6f}")
        log.info(f"   - ROUGE-L: {rouge_scores['rouge-l']:.6f}")
        log.info(f"   - ROUGE-avg: {rouge_scores['rouge-avg']:.6f}")
        log.info(f"")
        
        individual_scores.append({
            'model_index': i + 1,
            'model_metadata': metadata,
            'rouge_scores': rouge_scores
        })
    
    return individual_scores

def evaluate_hard_voting(models: List, tokenizers: List, configs: List, val_data: pd.DataFrame) -> Dict[str, float]:
    """í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” í‰ê°€"""
    log.info("ğŸ—³ï¸  í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
    
    input_texts = val_data['dialogue'].tolist()
    reference_summaries = val_data['summary'].tolist()
    remove_tokens = configs[0]['inference']['remove_tokens']
    tokenizer = tokenizers[0]
    
    # ê° ëª¨ë¸ë³„ë¡œ í† í° ID ìƒì„±
    all_generated_token_ids = []
    for i, (model, model_tokenizer, config) in enumerate(zip(models, tokenizers, configs)):
        model_token_ids = []
        for text in tqdm(input_texts, desc=f"í•˜ë“œë³´íŒ… - ëª¨ë¸ {i+1} í† í° ìƒì„±"):
            try:
                inputs = model_tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_ids = output_ids[0].tolist()
                model_token_ids.append(generated_ids)
                
            except Exception as e:
                log.warning(f"í•˜ë“œë³´íŒ… ëª¨ë¸ {i+1} ìƒ˜í”Œ ìƒì„± ì‹¤íŒ¨: {e}")
                model_token_ids.append([])
        
        all_generated_token_ids.append(model_token_ids)
    
    # í•˜ë“œ ë³´íŒ…ìœ¼ë¡œ ì•™ìƒë¸”
    predictions = []
    for sample_idx in tqdm(range(len(input_texts)), desc="í•˜ë“œë³´íŒ… ì•™ìƒë¸” ì²˜ë¦¬"):
        try:
            # ëª¨ë“  ëª¨ë¸ì˜ í† í° IDë“¤ì„ ìˆ˜ì§‘
            all_sequences = []
            for model_idx in range(len(models)):
                if sample_idx < len(all_generated_token_ids[model_idx]):
                    sequence = all_generated_token_ids[model_idx][sample_idx]
                    if sequence:  # ë¹ˆ ì‹œí€€ìŠ¤ê°€ ì•„ë‹ˆë©´
                        all_sequences.append(sequence)
            
            if not all_sequences:
                predictions.append("")
                continue
            
            # ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¾ê¸°
            max_length = max(len(seq) for seq in all_sequences)
            
            # ê° ìœ„ì¹˜ë³„ë¡œ ë‹¤ìˆ˜ê²° íˆ¬í‘œ
            ensemble_ids = []
            for pos in range(max_length):
                votes = []
                for seq in all_sequences:
                    if pos < len(seq):
                        votes.append(seq[pos])
                
                if votes:
                    # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ í† í° ì„ íƒ
                    token_counts = Counter(votes)
                    most_common_token = token_counts.most_common(1)[0][0]
                    ensemble_ids.append(most_common_token)
            
            # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            ensemble_text = tokenizer.decode(ensemble_ids, skip_special_tokens=True)
            for token in remove_tokens:
                ensemble_text = ensemble_text.replace(token, " ")
            
            predictions.append(ensemble_text.strip())
            
        except Exception as e:
            log.warning(f"í•˜ë“œë³´íŒ… ìƒ˜í”Œ {sample_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    # ë””ë²„ê¹…: ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸
    log.info(f"ğŸ” í•˜ë“œ ë³´íŒ… ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ì´ {len(predictions)}ê°œ):")
    for i in range(min(3, len(predictions))):
        log.info(f"  ì˜ˆì¸¡ {i+1}: '{predictions[i]}'")
        log.info(f"  ì°¸ì¡° {i+1}: '{reference_summaries[i]}'")
    
    # baseline.py ë°©ì‹ìœ¼ë¡œ ROUGE ê³„ì‚°
    return convert_text_predictions_to_baseline_format(
        predictions, reference_summaries, tokenizers[0], configs[0], "í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”"
    )

def evaluate_soft_voting(models: List, tokenizers: List, configs: List, val_data: pd.DataFrame) -> Dict[str, float]:
    """ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” í‰ê°€"""
    log.info("ğŸ¤ ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
    
    input_texts = val_data['dialogue'].tolist()
    reference_summaries = val_data['summary'].tolist()
    remove_tokens = configs[0]['inference']['remove_tokens']
    tokenizer = tokenizers[0]
    
    predictions = []
    
    for text in tqdm(input_texts, desc="ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì²˜ë¦¬"):
        try:
            # ê° ëª¨ë¸ì˜ beam search ê²°ê³¼ë“¤ ìˆ˜ì§‘
            all_candidates = []
            all_scores = []
            
            for model, model_tokenizer, config in zip(models, tokenizers, configs):
                inputs = model_tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping'],
                        return_dict_in_generate=True,
                        output_scores=True,
                        num_return_sequences=config['inference']['num_beams']
                    )
                
                # ê° í›„ë³´ë³„ë¡œ í…ìŠ¤íŠ¸ì™€ ì ìˆ˜ ì €ì¥
                for i, sequence in enumerate(outputs.sequences):
                    text_output = model_tokenizer.decode(sequence, skip_special_tokens=True)
                    for token in remove_tokens:
                        text_output = text_output.replace(token, " ")
                    
                    sequence_score = outputs.sequences_scores[i].item() if hasattr(outputs, 'sequences_scores') else 0.0
                    
                    all_candidates.append(text_output.strip())
                    all_scores.append(sequence_score)
            
            # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ í›„ë³´ ì„ íƒ
            if all_candidates and all_scores:
                best_idx = np.argmax(all_scores)
                best_candidate = all_candidates[best_idx]
                predictions.append(best_candidate)
            else:
                predictions.append("")
                
        except Exception as e:
            log.warning(f"ì†Œí”„íŠ¸ ë³´íŒ… ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    # baseline.py ë°©ì‹ìœ¼ë¡œ ROUGE ê³„ì‚°
    return convert_text_predictions_to_baseline_format(
        predictions, reference_summaries, tokenizers[0], configs[0], "ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”"
    )

def evaluate_length_based(models: List, tokenizers: List, configs: List, val_data: pd.DataFrame) -> Dict[str, float]:
    """ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” í‰ê°€ (baseline.py ë°©ì‹ ì‚¬ìš©)"""
    log.info("ğŸ“ ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
    
    input_texts = val_data['dialogue'].tolist()
    reference_summaries = val_data['summary'].tolist()
    
    predictions_ids = []
    labels_ids = []
    
    # ì°¸ì¡° ë‹µì•ˆì„ í† í° IDë¡œ ë³€í™˜
    tokenizer = tokenizers[0]
    for ref_text in reference_summaries:
        ref_tokens = tokenizer(ref_text, return_tensors="pt", truncation=True, padding=False, max_length=512)
        labels_ids.append(ref_tokens['input_ids'][0].tolist())
    
    for text in tqdm(input_texts, desc="ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì²˜ë¦¬"):
        try:
            candidates_ids = []
            
            # ê° ëª¨ë¸ì—ì„œ ìƒì„±
            for model, model_tokenizer, config in zip(models, tokenizers, configs):
                inputs = model_tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_ids = output_ids[0].tolist()
                candidates_ids.append(generated_ids)
            
            # ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ (í† í° ìˆ˜ ê¸°ì¤€)
            if candidates_ids:
                longest_candidate = max(candidates_ids, key=len)
                predictions_ids.append(longest_candidate)
            else:
                predictions_ids.append([tokenizer.pad_token_id])
                
        except Exception as e:
            log.warning(f"ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions_ids.append([tokenizer.pad_token_id])
    
    # baseline.pyì˜ compute_metrics ì‚¬ìš©í•˜ì—¬ ROUGE ê³„ì‚°
    rouge_scores = calculate_rouge_scores_with_baseline(
        predictions_ids, labels_ids, tokenizer, configs[0], "ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”"
    )
    return rouge_scores

def evaluate_logit_level(models: List, tokenizers: List, configs: List, val_data: pd.DataFrame) -> Dict[str, float]:
    """Logit ë ˆë²¨ ì•™ìƒë¸” í‰ê°€ (ìµœì í™”ëœ Beam Search + Nucleus Sampling)"""
    log.info("ğŸ¯ Logit ë ˆë²¨ ì•™ìƒë¸” ì‹œì‘...")
    
    input_texts = val_data['dialogue'].tolist()
    reference_summaries = val_data['summary'].tolist()
    remove_tokens = configs[0]['inference']['remove_tokens']
    tokenizer = tokenizers[0]
    config = configs[0]
    max_length = config['inference']['generate_max_length']
    num_beams = config['inference']['num_beams']
    device = models[0].device
    
    # Nucleus Sampling íŒŒë¼ë¯¸í„° ì¶”ê°€
    top_p = 1.0  # ensemble_inference_best.pyì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (Nucleus Sampling ë¹„í™œì„±í™”)
    
    predictions = []
    
    for idx, text in enumerate(tqdm(input_texts, desc="Logit ë ˆë²¨ ì•™ìƒë¸” ì²˜ë¦¬")):
        try:
            # ì…ë ¥ í† í°í™”
            inputs = tokenizer(
                text,
                return_tensors="pt",
                max_length=config['tokenizer']['encoder_max_len'],
                truncation=True,
                padding=True
            ).to(device)
            
            # ê° ëª¨ë¸ì˜ encoder ì¶œë ¥ ë¯¸ë¦¬ ê³„ì‚°
            encoder_outputs_list = []
            for model in models:
                with torch.no_grad():
                    encoder_outputs = model.get_encoder()(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask']
                    )
                    encoder_outputs_list.append(encoder_outputs.last_hidden_state.clone().detach())
            
            # Beam Search ì´ˆê¸°í™”
            decoder_start_token_id = tokenizer.bos_token_id
            if decoder_start_token_id is None:
                decoder_start_token_id = tokenizer.eos_token_id
            batch_size = 1
            beam_size = num_beams
            
            sequences = torch.full((batch_size * beam_size, 1), decoder_start_token_id, device=device)
            beam_scores = torch.zeros(batch_size * beam_size, device=device)
            beam_scores[1:] = -float('inf')
            
            eos_token_id = tokenizer.eos_token_id
            finished_sequences = []
            
            # Beam Search ë£¨í”„
            for step in range(max_length - 1):
                if len(finished_sequences) >= beam_size:
                    break
                
                current_sequences = sequences[beam_scores > -float('inf')]
                current_scores = beam_scores[beam_scores > -float('inf')]
                
                if len(current_sequences) == 0:
                    break
                
                # ê° ëª¨ë¸ì—ì„œ logits ê³„ì‚°
                all_next_logits = []
                for model_idx, model in enumerate(models):
                    try:
                        with torch.no_grad():
                            decoder_outputs = model.get_decoder()(
                                input_ids=current_sequences,
                                encoder_hidden_states=encoder_outputs_list[model_idx].expand(len(current_sequences), -1, -1),
                                encoder_attention_mask=inputs['attention_mask'].expand(len(current_sequences), -1)
                            )
                            
                            logits = model.lm_head(decoder_outputs.last_hidden_state)
                            next_token_logits = logits[:, -1, :]
                            all_next_logits.append(next_token_logits)
                    except Exception as e:
                        log.warning(f"ëª¨ë¸ {model_idx} ìŠ¤í… {step} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì²˜ë¦¬
                if not all_next_logits:
                    log.warning(f"ìƒ˜í”Œ {idx}: ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨, ë¹ˆ ë¬¸ìì—´ ë°˜í™˜")
                    predictions.append("")
                    break
                
                # ëª¨ë“  ëª¨ë¸ì˜ logits í‰ê· 
                ensemble_logits = torch.stack(all_next_logits).mean(dim=0)
                
                # Nucleus Sampling (Top-p) ì ìš©
                if top_p < 1.0:
                    for beam_idx in range(ensemble_logits.size(0)):
                        sorted_logits, sorted_indices = torch.sort(ensemble_logits[beam_idx], descending=True)
                        sorted_probs = torch.softmax(sorted_logits, dim=-1)
                        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                        
                        # ëˆ„ì  í™•ë¥ ì´ top_pë¥¼ ì´ˆê³¼í•˜ëŠ” í† í°ë“¤ ì œê±°
                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                        sorted_indices_to_remove[0] = 0
                        
                        indices_to_remove = sorted_indices[sorted_indices_to_remove]
                        ensemble_logits[beam_idx, indices_to_remove] = -float('inf')
                        
                        # ëª¨ë“  í† í°ì´ ì œê±°ëœ ê²½ìš° ì‘ê¸‰ì²˜ì¹˜ (best.pyì™€ ë™ì¼)
                        valid_tokens = (ensemble_logits[beam_idx] > -float('inf')).sum().item()
                        if valid_tokens == 0:
                            # ìµœê³  í™•ë¥  í† í° í•˜ë‚˜ëŠ” ìœ ì§€
                            best_token_idx = torch.argmax(sorted_logits)
                            ensemble_logits[beam_idx, sorted_indices[best_token_idx]] = sorted_logits[best_token_idx]
                
                # Log probabilities ê³„ì‚°
                next_token_log_probs = torch.log_softmax(ensemble_logits, dim=-1)
                
                # ìƒˆë¡œìš´ beam í›„ë³´ ìƒì„±
                vocab_size = next_token_log_probs.size(-1)
                next_scores = current_scores.unsqueeze(1) + next_token_log_probs
                next_scores = next_scores.view(-1)
                
                # Top-k ì„ íƒ
                top_scores, top_indices = torch.topk(next_scores, min(beam_size * 2, len(next_scores)))
                
                # ìƒˆ beam êµ¬ì„±
                new_sequences = []
                new_scores = []
                
                for score, token_idx in zip(top_scores, top_indices):
                    beam_idx = token_idx // vocab_size
                    token_id = token_idx % vocab_size
                    
                    new_seq = torch.cat([
                        current_sequences[beam_idx],
                        torch.tensor([token_id], device=device)
                    ])
                    
                    # EOS í† í° ì²´í¬
                    if token_id == eos_token_id:
                        finished_sequences.append((new_seq, score.item()))
                    else:
                        new_sequences.append(new_seq)
                        new_scores.append(score)
                        
                    if len(new_sequences) >= beam_size:
                        break
                
                if not new_sequences:
                    break
                
                # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì—…ë°ì´íŠ¸
                max_len = max(len(seq) for seq in new_sequences)
                sequences = torch.full((beam_size, max_len), tokenizer.pad_token_id, device=device)
                beam_scores = torch.full((beam_size,), -float('inf'), device=device)
                
                for i, (seq, score) in enumerate(zip(new_sequences[:beam_size], new_scores[:beam_size])):
                    sequences[i, :len(seq)] = seq
                    beam_scores[i] = score
            
            # ìµœê³  ì ìˆ˜ ì‹œí€€ìŠ¤ ì„ íƒ
            if finished_sequences:
                best_sequence, best_score = max(finished_sequences, key=lambda x: x[1])
            else:
                best_idx = torch.argmax(beam_scores)
                best_sequence = sequences[best_idx]
            
            # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            generated_text = tokenizer.decode(best_sequence, skip_special_tokens=False)
            for token in remove_tokens:
                generated_text = generated_text.replace(token, " ")
            
            predictions.append(generated_text.strip())
            
        except Exception as e:
            log.warning(f"Logit ë ˆë²¨ ì•™ìƒë¸” ìƒ˜í”Œ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    # baseline.py ë°©ì‹ìœ¼ë¡œ ROUGE ê³„ì‚°
    return convert_text_predictions_to_baseline_format(
        predictions, reference_summaries, tokenizers[0], configs[0], "Logit ë ˆë²¨ ì•™ìƒë¸”"
    )

def evaluate_realtime_token(models: List, tokenizers: List, configs: List, val_data: pd.DataFrame) -> Dict[str, float]:
    """ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” í‰ê°€"""
    log.info("âš¡ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì‹œì‘...")
    
    input_texts = val_data['dialogue'].tolist()
    reference_summaries = val_data['summary'].tolist()
    remove_tokens = configs[0]['inference']['remove_tokens']
    tokenizer = tokenizers[0]
    config = configs[0]
    max_length = config['inference']['generate_max_length']
    device = models[0].device
    
    predictions = []
    
    for text in tqdm(input_texts, desc="ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì²˜ë¦¬"):
        try:
            # ì…ë ¥ í† í°í™”
            inputs = tokenizer(
                text,
                return_tensors="pt",
                max_length=config['tokenizer']['encoder_max_len'],
                truncation=True,
                padding=True
            ).to(device)
            
            # ê° ëª¨ë¸ì˜ encoder ì¶œë ¥ ë¯¸ë¦¬ ê³„ì‚°
            model_encoder_outputs = []
            for model in models:
                with torch.no_grad():
                    encoder_outputs = model.get_encoder()(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask']
                    )
                    model_encoder_outputs.append(encoder_outputs.last_hidden_state)
            
            # ì‹œì‘ í† í°ìœ¼ë¡œ ì´ˆê¸°í™”
            decoder_start_token_id = tokenizer.bos_token_id
            if decoder_start_token_id is None:
                decoder_start_token_id = tokenizer.eos_token_id
            generated_sequence = [decoder_start_token_id]
            eos_token_id = tokenizer.eos_token_id
            
            # í† í°ë³„ ìƒì„± ë£¨í”„
            for step in range(max_length - 1):
                current_ids = torch.tensor([generated_sequence], device=device)
                
                # ê° ëª¨ë¸ì—ì„œ ë‹¤ìŒ í† í° logits ê³„ì‚°
                model_logits = []
                for i, model in enumerate(models):
                    try:
                        with torch.no_grad():
                            decoder_outputs = model.get_decoder()(
                                input_ids=current_ids,
                                encoder_hidden_states=model_encoder_outputs[i],
                                encoder_attention_mask=inputs['attention_mask']
                            )
                            
                            logits = model.lm_head(decoder_outputs.last_hidden_state)
                            next_token_logits = logits[0, -1, :]
                            model_logits.append(next_token_logits)
                            
                    except Exception as e:
                        log.warning(f"ëª¨ë¸ {i+1} ìŠ¤í… {step} ì˜¤ë¥˜: {e}")
                        continue
                
                if not model_logits:
                    break
                
                # ëª¨ë“  ëª¨ë¸ì˜ logits í‰ê· 
                ensemble_logits = torch.stack(model_logits).mean(dim=0)
                
                # ë‹¤ìŒ í† í° ì„ íƒ (greedy decoding)
                next_token_id = torch.argmax(ensemble_logits).item()
                
                # EOS í† í°ì´ë©´ ì¢…ë£Œ
                if next_token_id == eos_token_id:
                    break
                
                generated_sequence.append(next_token_id)
            
            # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)
            for token in remove_tokens:
                generated_text = generated_text.replace(token, " ")
            
            predictions.append(generated_text.strip())
            
        except Exception as e:
            log.warning(f"ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    # baseline.py ë°©ì‹ìœ¼ë¡œ ROUGE ê³„ì‚°
    return convert_text_predictions_to_baseline_format(
        predictions, reference_summaries, tokenizers[0], configs[0], "ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”"
    )

# =============================================================================
# ë©”ì¸ ì‹¤í—˜ í•¨ìˆ˜
# =============================================================================

def main_comprehensive_experiment():
    """
    ğŸ”¬ ë‹¤ì„¯ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ì¢…í•© ë¹„êµ ì‹¤í—˜
    ê° ë°©ì‹ì´ ì™„ë£Œë  ë•Œë§ˆë‹¤ ì¦‰ì‹œ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    log.info("ğŸ”¬ " + "="*60)
    log.info("ğŸ¯ ë‹¤ì„¯ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ì¢…í•© ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    log.info("="*60)
    
    # ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    model_paths = get_model_paths()
    if not model_paths:
        log.error("ğŸ’¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    log.info(f"ğŸš€ ì´ {len(model_paths)}ê°œ ëª¨ë¸ë¡œ ì‹¤í—˜ ì§„í–‰")
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    val_data_path = "../../input/data/dev.csv"
    if not os.path.exists(val_data_path):
        log.error(f"ğŸ’¥ ê²€ì¦ ë°ì´í„° ì—†ìŒ: {val_data_path}")
        return
    
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ
    log.info("ğŸ“Š ê²€ì¦ ë°ì´í„° ë¡œë“œ ì¤‘...")
    val_data = pd.read_csv(val_data_path)
    # DEV_DATA_LIMITì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
    if DEV_DATA_LIMIT is not None and DEV_DATA_LIMIT > 0:
        val_data = val_data.head(DEV_DATA_LIMIT)
    log.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(val_data)}ê°œ ìƒ˜í”Œ")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_data_path = "../../input/data/test.csv"
    test_data = None
    if os.path.exists(test_data_path):
        test_data = pd.read_csv(test_data_path)
        # TEST_DATA_LIMITì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
        if TEST_DATA_LIMIT is not None and TEST_DATA_LIMIT > 0:
            test_data = test_data.head(TEST_DATA_LIMIT)
        log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(test_data)}ê°œ ìƒ˜í”Œ")
    else:
        log.warning(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_data_path}")
    
    # ëª¨ë¸ ë¡œë”©
    log.info("ğŸ¤– ëª¨ë¸ë“¤ ë¡œë”© ì¤‘...")
    models, tokenizers, configs, metadata_list = load_models(model_paths, device)
    if not models:
        log.error("ğŸ’¥ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨!")
        return
    
    # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
    experiment_results = {
        'timestamp': timestamp,
        'model_paths': model_paths,
        'device': device,
        'validation_samples': len(val_data),
        'methods': {},
        'individual_model_scores': [],
        'performance_ranking': [],
        'time_ranking': []
    }
    
    log.info("")
    log.info("ğŸš€ ì‹¤í—˜ ì‹œì‘! ê° ë°©ì‹ì´ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ì ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.")
    log.info("="*60)
    
    # 1. ê°œë³„ ëª¨ë¸ í‰ê°€
    log.info("1ï¸âƒ£ ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    log.info("-"*30)
    start_time = time.time()
    individual_scores = evaluate_individual_models(models, tokenizers, configs, metadata_list, val_data)
    individual_time = time.time() - start_time
    experiment_results['individual_model_scores'] = individual_scores
    log.info(f"â±ï¸  ê°œë³„ ëª¨ë¸ í‰ê°€ ì™„ë£Œ: {individual_time:.2f}ì´ˆ")
    log.info("")
    
    # 2. í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”
    log.info("2ï¸âƒ£ í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”")
    log.info("-"*30)
    start_time = time.time()
    hard_voting_scores = evaluate_hard_voting(models, tokenizers, configs, val_data)
    hard_voting_time = time.time() - start_time
    experiment_results['methods']['hard_voting'] = {
        'rouge_scores': hard_voting_scores,
        'time_seconds': hard_voting_time,
        'method_type': 'Post-processing'
    }
    log.info(f"â±ï¸  í•˜ë“œ ë³´íŒ… ì™„ë£Œ: {hard_voting_time:.2f}ì´ˆ")
    
    # í•˜ë“œ ë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
    if test_data is not None:
        csv_path = run_test_inference_for_method(models, tokenizers, configs, "hard_voting", test_data, timestamp)
        experiment_results['methods']['hard_voting']['test_csv_path'] = csv_path
    
    log.info("")
    
    # 3. ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”
    log.info("3ï¸âƒ£ ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”")
    log.info("-"*30)
    start_time = time.time()
    soft_voting_scores = evaluate_soft_voting(models, tokenizers, configs, val_data)
    soft_voting_time = time.time() - start_time
    experiment_results['methods']['soft_voting'] = {
        'rouge_scores': soft_voting_scores,
        'time_seconds': soft_voting_time,
        'method_type': 'Post-processing'
    }
    log.info(f"â±ï¸  ì†Œí”„íŠ¸ ë³´íŒ… ì™„ë£Œ: {soft_voting_time:.2f}ì´ˆ")
    
    # ì†Œí”„íŠ¸ ë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
    if test_data is not None:
        csv_path = run_test_inference_for_method(models, tokenizers, configs, "soft_voting", test_data, timestamp)
        experiment_results['methods']['soft_voting']['test_csv_path'] = csv_path
    
    log.info("")
    
    # 4. ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”
    log.info("4ï¸âƒ£ ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”")
    log.info("-"*30)
    start_time = time.time()
    length_based_scores = evaluate_length_based(models, tokenizers, configs, val_data)
    length_based_time = time.time() - start_time
    experiment_results['methods']['length_based'] = {
        'rouge_scores': length_based_scores,
        'time_seconds': length_based_time,
        'method_type': 'Post-processing'
    }
    log.info(f"â±ï¸  ê¸¸ì´ ê¸°ë°˜ ì™„ë£Œ: {length_based_time:.2f}ì´ˆ")
    
    # ê¸¸ì´ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
    if test_data is not None:
        csv_path = run_test_inference_for_method(models, tokenizers, configs, "length_based", test_data, timestamp)
        experiment_results['methods']['length_based']['test_csv_path'] = csv_path
    
    log.info("")
    
    # 5. Logit ë ˆë²¨ ì•™ìƒë¸”
    log.info("5ï¸âƒ£ Logit ë ˆë²¨ ì•™ìƒë¸”")
    log.info("-"*30)
    start_time = time.time()
    logit_level_scores = evaluate_logit_level(models, tokenizers, configs, val_data)
    logit_level_time = time.time() - start_time
    experiment_results['methods']['logit_level'] = {
        'rouge_scores': logit_level_scores,
        'time_seconds': logit_level_time,
        'method_type': 'Runtime'
    }
    log.info(f"â±ï¸  Logit ë ˆë²¨ ì™„ë£Œ: {logit_level_time:.2f}ì´ˆ")
    
    # Logit ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
    if test_data is not None:
        csv_path = run_test_inference_for_method(models, tokenizers, configs, "logit_level", test_data, timestamp)
        experiment_results['methods']['logit_level']['test_csv_path'] = csv_path
    
    log.info("")
    
    # 6. ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”
    log.info("6ï¸âƒ£ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”")
    log.info("-"*30)
    start_time = time.time()
    realtime_token_scores = evaluate_realtime_token(models, tokenizers, configs, val_data)
    realtime_token_time = time.time() - start_time
    experiment_results['methods']['realtime_token_ensemble'] = {
        'rouge_scores': realtime_token_scores,
        'time_seconds': realtime_token_time,
        'method_type': 'Runtime'
    }
    log.info(f"â±ï¸  ì‹¤ì‹œê°„ í† í° ì™„ë£Œ: {realtime_token_time:.2f}ì´ˆ")
    
    # ì‹¤ì‹œê°„ í† í° í…ŒìŠ¤íŠ¸ ì¶”ë¡ 
    if test_data is not None:
        csv_path = run_test_inference_for_method(models, tokenizers, configs, "realtime_token_ensemble", test_data, timestamp)
        experiment_results['methods']['realtime_token_ensemble']['test_csv_path'] = csv_path
    
    log.info("")
    
    # ìµœì¢… ë¹„êµ ë° ìˆœìœ„ ì •ë¦¬
    log.info("ğŸ† ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    log.info("="*60)
    
    # ì„±ëŠ¥ ìˆœìœ„ (ROUGE-avg ê¸°ì¤€)
    performance_ranking = []
    for method_name, method_data in experiment_results['methods'].items():
        rouge_avg = method_data['rouge_scores']['rouge-avg']
        time_seconds = method_data['time_seconds']
        performance_ranking.append((method_name, rouge_avg, time_seconds))
    
    performance_ranking.sort(key=lambda x: x[1], reverse=True)
    experiment_results['performance_ranking'] = performance_ranking
    
    # ì‹œê°„ ìˆœìœ„ (ì‹¤í–‰ ì‹œê°„ ê¸°ì¤€)
    time_ranking = sorted(performance_ranking, key=lambda x: x[2])
    experiment_results['time_ranking'] = time_ranking
    
    # ê²°ê³¼ ì¶œë ¥
    log.info("ğŸ“Š ì„±ëŠ¥ ìˆœìœ„ (ROUGE-avg ê¸°ì¤€):")
    for i, (method, rouge_avg, time_sec) in enumerate(performance_ranking, 1):
        log.info(f"  {i}ìœ„: {method:<20} ROUGE-avg: {rouge_avg:.6f} ({time_sec:.1f}ì´ˆ)")
    
    log.info("")
    log.info("â±ï¸ ì‹¤í–‰ ì‹œê°„ ìˆœìœ„:")
    for i, (method, rouge_avg, time_sec) in enumerate(time_ranking, 1):
        log.info(f"  {i}ìœ„: {method:<20} {time_sec:.1f}ì´ˆ (ROUGE-avg: {rouge_avg:.6f})")
    
    # ìµœê³  ì„±ëŠ¥ ë°©ë²•ì˜ CSVë¥¼ ìµœì¢… ì œì¶œìš©ìœ¼ë¡œ ë³µì‚¬
    if test_data is not None and performance_ranking:
        best_method = performance_ranking[0][0]
        best_csv_path = experiment_results['methods'][best_method].get('test_csv_path')
        if best_csv_path:
            final_csv_path = os.path.join("./prediction", f"best_{best_method}_{timestamp}.csv")
            import shutil
            shutil.copy2(best_csv_path, final_csv_path)
            log.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ë°©ë²• ({best_method}) ê²°ê³¼ë¥¼ ìµœì¢… ì œì¶œìš©ìœ¼ë¡œ ì €ì¥: {final_csv_path}")
    
    # ê²°ê³¼ ì €ì¥
    json_path = save_results_to_json(experiment_results, timestamp)
    
    log.info("")
    log.info("âœ… ëª¨ë“  ì•™ìƒë¸” ì‹¤í—˜ ì™„ë£Œ!")
    log.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {json_path}")
    if test_data is not None:
        log.info("ğŸ“ í…ŒìŠ¤íŠ¸ ì¶”ë¡  ê²°ê³¼ëŠ” ./prediction í´ë”ì— ì €ì¥ë¨")
    log.info("="*60)
    
    return experiment_results

def run_single_method(method_name: str):
    """ë‹¨ì¼ ì•™ìƒë¸” ë°©ì‹ ì‹¤í–‰"""
    log.info(f"ğŸ¯ {method_name} ë‹¨ì¼ ë°©ì‹ ì‹¤í–‰ ëª¨ë“œ")
    
    # ëª¨ë¸ ë¡œë”©
    model_paths = get_model_paths()
    if not model_paths:
        return
    
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    models, tokenizers, configs, metadata_list = load_models(model_paths, device)
    if not models:
        return
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ
    val_data_path = "../../input/data/dev.csv"
    if not os.path.exists(val_data_path):
        log.error(f"ê²€ì¦ ë°ì´í„° ì—†ìŒ: {val_data_path}")
        return
    
    val_data = pd.read_csv(val_data_path)
    # DEV_DATA_LIMITì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ í•´ë‹¹ ê°œìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
    if DEV_DATA_LIMIT is not None and DEV_DATA_LIMIT > 0:
        val_data = val_data.head(DEV_DATA_LIMIT)
    log.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(val_data)}ê°œ ìƒ˜í”Œ")
    
    # í•´ë‹¹ ë°©ì‹ ì‹¤í–‰
    start_time = time.time()
    
    if method_name == "hard_voting":
        scores = evaluate_hard_voting(models, tokenizers, configs, val_data)
    elif method_name == "soft_voting":
        scores = evaluate_soft_voting(models, tokenizers, configs, val_data)
    elif method_name == "length_based":
        scores = evaluate_length_based(models, tokenizers, configs, val_data)
    elif method_name == "logit_level":
        scores = evaluate_logit_level(models, tokenizers, configs, val_data)
    elif method_name == "realtime_token":
        scores = evaluate_realtime_token(models, tokenizers, configs, val_data)
    else:
        log.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°©ì‹: {method_name}")
        return
    
    elapsed_time = time.time() - start_time
    log.info(f"âœ… {method_name} ì‹¤í–‰ ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description='ëŒ€í™” ìš”ì•½ ì•™ìƒë¸” ì¶”ë¡  (ê°„ì†Œí™”ëœ í•¨ìˆ˜ ê¸°ë°˜ ë²„ì „)')
    
    parser.add_argument(
        '--mode', 
        type=str, 
        default='all',
        choices=['all', 'hard_voting', 'soft_voting', 'length_based', 'realtime_token', 'logit_level'],
        help='ì‹¤í–‰í•  ì•™ìƒë¸” ë°©ì‹ ì„ íƒ (ê¸°ë³¸ê°’: all - ëª¨ë“  ë°©ì‹ ë¹„êµ)'
    )
    
    args = parser.parse_args()
    
    log.info("ğŸ”¬ ëŒ€í™” ìš”ì•½ ì•™ìƒë¸” ì¶”ë¡  ì‹œìŠ¤í…œ ì‹œì‘")
    log.info(f"ì„ íƒëœ ëª¨ë“œ: {args.mode}")
    
    if args.mode == "all":
        main_comprehensive_experiment()
    else:
        run_single_method(args.mode)

# =============================================================================
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  í•¨ìˆ˜ë“¤
# =============================================================================

def test_inference_hard_voting(models: List, tokenizers: List, configs: List, input_texts: List[str], remove_tokens: List[str]) -> List[str]:
    """í•˜ë“œ ë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡ """
    predictions = []
    
    for i, input_text in enumerate(tqdm(input_texts, desc="í•˜ë“œë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡ ")):
        try:
            model_token_ids = []
            
            for model_idx, (model, tokenizer, config) in enumerate(zip(models, tokenizers, configs)):
                inputs = tokenizer(
                    input_text,
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_ids = output_ids[0].tolist()
                model_token_ids.append(generated_ids)
            
            # í•˜ë“œ ë³´íŒ…: ê° ìœ„ì¹˜ë³„ ìµœë¹ˆê°’ ì„ íƒ
            max_len = max(len(ids) for ids in model_token_ids)
            
            ensemble_ids = []
            for pos in range(max_len):
                position_tokens = []
                for ids in model_token_ids:
                    if pos < len(ids):
                        position_tokens.append(ids[pos])
                
                if position_tokens:
                    ensemble_ids.append(max(set(position_tokens), key=position_tokens.count))
            
            # í…ìŠ¤íŠ¸ ë””ì½”ë”©
            ensemble_text = tokenizers[0].decode(ensemble_ids, skip_special_tokens=True)
            for token in remove_tokens:
                ensemble_text = ensemble_text.replace(token, " ")
            
            predictions.append(ensemble_text.strip())
            
        except Exception as e:
            log.warning(f"í•˜ë“œë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    return predictions

def test_inference_soft_voting(models: List, tokenizers: List, configs: List, input_texts: List[str], remove_tokens: List[str]) -> List[str]:
    """ì†Œí”„íŠ¸ ë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡ """
    predictions = []
    
    for input_text in tqdm(input_texts, desc="ì†Œí”„íŠ¸ë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡ "):
        try:
            model_outputs = []
            
            for model, tokenizer, config in zip(models, tokenizers, configs):
                inputs = tokenizer(
                    input_text,
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping'],
                        return_dict_in_generate=True,
                        output_scores=True,
                        num_return_sequences=config['inference']['num_beams']
                    )
                
                for sequence in outputs.sequences:
                    text_output = tokenizer.decode(sequence, skip_special_tokens=True)
                    for token in remove_tokens:
                        text_output = text_output.replace(token, " ")
                    model_outputs.append(text_output.strip())
            
            # ì†Œí”„íŠ¸ ë³´íŒ…: í‰ê·  ìŠ¤ì½”ì–´ê°€ ê°€ì¥ ë†’ì€ ê²°ê³¼ ì„ íƒ
            if model_outputs:
                predictions.append(model_outputs[0])  # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
            else:
                predictions.append("")
                
        except Exception as e:
            log.warning(f"ì†Œí”„íŠ¸ë³´íŒ… í…ŒìŠ¤íŠ¸ ì¶”ë¡  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    return predictions

def test_inference_length_based(models: List, tokenizers: List, configs: List, input_texts: List[str], remove_tokens: List[str]) -> List[str]:
    """ê¸¸ì´ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì¶”ë¡ """
    predictions = []
    
    for input_text in tqdm(input_texts, desc="ê¸¸ì´ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì¶”ë¡ "):
        try:
            candidates = []
            
            for model, tokenizer, config in zip(models, tokenizers, configs):
                inputs = tokenizer(
                    input_text,
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(model.device)
                
                with torch.no_grad():
                    output_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                for token in remove_tokens:
                    generated_text = generated_text.replace(token, " ")
                
                candidates.append(generated_text.strip())
            
            # ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
            if candidates:
                longest_candidate = max(candidates, key=len)
                predictions.append(longest_candidate)
            else:
                predictions.append("")
                
        except Exception as e:
            log.warning(f"ê¸¸ì´ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ì¶”ë¡  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    return predictions

def test_inference_logit_level(models: List, tokenizers: List, configs: List, input_texts: List[str], remove_tokens: List[str]) -> List[str]:
    """Logit ë ˆë²¨ í…ŒìŠ¤íŠ¸ ì¶”ë¡ """
    predictions = []
    
    for input_text in tqdm(input_texts, desc="Logitë ˆë²¨ í…ŒìŠ¤íŠ¸ ì¶”ë¡ "):
        try:
            inputs = tokenizers[0](
                input_text,
                max_length=configs[0]['inference']['max_length'],
                truncation=True,
                padding=True
            ).to(models[0].device)
            
            # ë¹” ì„œì¹˜ íŒŒë¼ë¯¸í„°
            max_length = configs[0]['inference']['generate_max_length']
            num_beams = configs[0]['inference']['num_beams']
            
            # ì´ˆê¸° ì‹œí€€ìŠ¤
            batch_size = inputs['input_ids'].size(0)
            device = inputs['input_ids'].device
            
            sequences = inputs['input_ids'].clone()
            scores = torch.zeros(batch_size, device=device)
            finished = torch.zeros(batch_size, dtype=torch.bool, device=device)
            
            # ìƒì„± ë£¨í”„
            for step in range(max_length - sequences.size(1)):
                if finished.all():
                    break
                
                # ëª¨ë“  ëª¨ë¸ì˜ logit ìˆ˜ì§‘
                ensemble_logits = None
                
                for model in models:
                    with torch.no_grad():
                        outputs = model(input_ids=sequences, attention_mask=torch.ones_like(sequences))
                        current_logits = outputs.logits[:, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ì˜ logits
                        
                        if ensemble_logits is None:
                            ensemble_logits = current_logits.clone()
                        else:
                            ensemble_logits += current_logits
                
                # í‰ê·  logits ê³„ì‚°
                ensemble_logits = ensemble_logits / len(models)
                
                # ë‹¤ìŒ í† í° ì˜ˆì¸¡
                next_token_logits = ensemble_logits
                next_tokens = torch.argmax(next_token_logits, dim=-1)
                
                # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
                sequences = torch.cat([sequences, next_tokens.unsqueeze(-1)], dim=-1)
                
                # EOS í† í° ì²´í¬
                eos_token_id = tokenizers[0].eos_token_id
                if eos_token_id is not None:
                    finished = finished | (next_tokens == eos_token_id)
            
            # ìµœì¢… ì‹œí€€ìŠ¤ì—ì„œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            generated_sequence = sequences[0][inputs['input_ids'].size(1):]
            generated_text = tokenizers[0].decode(generated_sequence, skip_special_tokens=True)
            
            for token in remove_tokens:
                generated_text = generated_text.replace(token, " ")
            
            predictions.append(generated_text.strip())
            
        except Exception as e:
            log.warning(f"Logitë ˆë²¨ í…ŒìŠ¤íŠ¸ ì¶”ë¡  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    return predictions

def test_inference_realtime_token(models: List, tokenizers: List, configs: List, input_texts: List[str], remove_tokens: List[str]) -> List[str]:
    """ì‹¤ì‹œê°„ í† í° í…ŒìŠ¤íŠ¸ ì¶”ë¡ """
    predictions = []
    
    for input_text in tqdm(input_texts, desc="ì‹¤ì‹œê°„í† í° í…ŒìŠ¤íŠ¸ ì¶”ë¡ "):
        try:
            inputs = tokenizers[0](
                input_text,
                max_length=configs[0]['inference']['max_length'],
                truncation=True,
                padding=True
            ).to(models[0].device)
            
            max_length = configs[0]['inference']['generate_max_length']
            sequences = inputs['input_ids'].clone()
            device = inputs['input_ids'].device
            
            # í† í°ë³„ ìƒì„±
            for step in range(max_length - sequences.size(1)):
                model_predictions = []
                
                # ê° ëª¨ë¸ì˜ ë‹¤ìŒ í† í° ì˜ˆì¸¡
                for model in models:
                    with torch.no_grad():
                        outputs = model(input_ids=sequences)
                        next_token_logits = outputs.logits[0, -1, :]
                        next_token = torch.argmax(next_token_logits).item()
                        model_predictions.append(next_token)
                
                # ë‹¤ìˆ˜ê²° íˆ¬í‘œ
                if model_predictions:
                    ensemble_token = max(set(model_predictions), key=model_predictions.count)
                    sequences = torch.cat([sequences, torch.tensor([[ensemble_token]], device=device)], dim=1)
                    
                    # EOS í† í° ì²´í¬
                    if ensemble_token == tokenizers[0].eos_token_id:
                        break
                else:
                    break
            
            # ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©
            generated_sequence = sequences[0][inputs['input_ids'].size(1):]
            generated_text = tokenizers[0].decode(generated_sequence, skip_special_tokens=True)
            
            for token in remove_tokens:
                generated_text = generated_text.replace(token, " ")
            
            predictions.append(generated_text.strip())
            
        except Exception as e:
            log.warning(f"ì‹¤ì‹œê°„í† í° í…ŒìŠ¤íŠ¸ ì¶”ë¡  ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            predictions.append("")
    
    return predictions

if __name__ == "__main__":
    main()