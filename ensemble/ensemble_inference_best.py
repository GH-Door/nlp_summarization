#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Ensemble Inference System for Multiple Strategies
WandB sweepìœ¼ë¡œ ìƒì„±ëœ ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ì‹œìŠ¤í…œ

ì§€ì›í•˜ëŠ” ì•™ìƒë¸” ë°©ì‹:
1. í•˜ë“œ ë³´íŒ… (Hard Voting): í† í°ë³„ ë‹¤ìˆ˜ê²°
2. ì†Œí”„íŠ¸ ë³´íŒ… (Soft Voting): í™•ë¥  ë¶„í¬ í‰ê·   
3. ê¸¸ì´ ê¸°ë°˜ (Length-based): ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
4. ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” (Realtime Token Ensemble): ë§¤ í† í°ë§ˆë‹¤ í™•ë¥  ë¶„í¬ í‰ê· 
5. Logit ë ˆë²¨ ì•™ìƒë¸” (Logit Level Ensemble): ìµœì í™”ëœ Nucleus Sampling + Beam Search

ì‚¬ìš©ë²•:
- python ensemble_inference.py --mode=all           # ëª¨ë“  ë°©ì‹ ë¹„êµ
- python ensemble_inference.py --mode=hard_voting   # í•˜ë“œ ë³´íŒ…ë§Œ
- python ensemble_inference.py --mode=soft_voting   # ì†Œí”„íŠ¸ ë³´íŒ…ë§Œ
- python ensemble_inference.py --mode=length_based  # ê¸¸ì´ ê¸°ë°˜ë§Œ
- python ensemble_inference.py --mode=realtime_token # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”ë§Œ
- python ensemble_inference.py --mode=logit_level    # ìµœì í™”ëœ Logit ì•™ìƒë¸”ë§Œ
"""

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ë¥¼ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì„¤ì •
import os; os.chdir(os.path.dirname(os.path.abspath(__file__)))
import sys; sys.path.append('../utils')
import log_util as log

import pandas as pd
import json
import yaml
import torch
import zipfile
import shutil
import time
from datetime import datetime
from collections import Counter
from tqdm import tqdm
import random
import numpy as np

from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig

# baseline.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ ì„í¬íŠ¸
from baseline import Preprocess, DatasetForVal, compute_metrics
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
import tempfile

def get_model_paths():
    """
    ëª¨ë¸ ê²½ë¡œë“¤ì„ ë°˜í™˜í•˜ëŠ” ê³µí†µ í•¨ìˆ˜
    
    Returns:
        list: ì¡´ì¬í•˜ëŠ” ëª¨ë¸ íŒŒì¼ ê²½ë¡œë“¤
    """
    # TODO: ì‹¤ì œ ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”
    model_paths = [
        "./models/model_baseline_20250805_070447.zip",  
        "./models/model_baseline_20250805_060913.zip",
        "./models/model_baseline_20250805_094805.zip",
        # ì¶”ê°€
        "./models/model_baseline_20250805_234915.zip",
        "./models/model_baseline_20250806_033243.zip",
        # "./models/model_baseline_20250805_070447.zip",
        "./models/model_baseline_20250805_191209.zip",
        "./models/model_baseline_20250805_173237.zip",
        "./models/model_baseline_20250805_183711.zip",
    ]
    
    # ì¡´ì¬í•˜ëŠ” ëª¨ë¸ íŒŒì¼ë§Œ í•„í„°ë§
    existing_model_paths = []
    for path in model_paths:
        if os.path.exists(path):
            existing_model_paths.append(path)
            log.info(f"ëª¨ë¸ íŒŒì¼ í™•ì¸: {path}")
        else:
            log.warning(f"ëª¨ë¸ íŒŒì¼ ì—†ìŒ (ê±´ë„ˆëœ€): {path}")
    
    if not existing_model_paths:
        log.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        log.info("ë¨¼ì € WandB sweepì„ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ì„¸ìš”:")
        log.info("python wandb_sweep.py --count 3")
        return []
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
    return existing_model_paths

def load_model_package(zip_path):
    """
    ZIP íŒŒì¼ì—ì„œ ëª¨ë¸, í† í¬ë‚˜ì´ì €, ì„¤ì •ì„ ë¡œë”©
    
    Args:
        zip_path: ZIP íŒŒì¼ ê²½ë¡œ
        
    Returns:
        tuple: (model, tokenizer, config, metadata)
    """
    temp_dir = f"temp_load_{int(time.time())}"
    
    try:
        log.info(f"ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë”© ì‹œì‘: {zip_path}")
        
        # ZIP ì••ì¶• í•´ì œ
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        
        # ì„¤ì • ë¡œë“œ
        config_path = os.path.join(temp_dir, "config.yaml")
        with open(config_path, "r", encoding='utf-8') as f:
            config = yaml.safe_load(f)
        log.info("ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = os.path.join(temp_dir, "metadata.json")
        with open(metadata_path, "r", encoding='utf-8') as f:
            metadata = json.load(f)
        log.info("ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # ì‹œë“œ ì„¤ì • (ì¬í˜„ì„± ë³´ì¥)
        if 'training' in config and 'seed' in config['training']:
            seed = config['training']['seed']
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            np.random.seed(seed)
            random.seed(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            log.info(f"ëª¨ë¸ ë¡œë”© ì‹œ ì‹œë“œ ì„¤ì •: {seed}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ ë¨¼ì € (ì‹¤ì œ vocab í¬ê¸° í™•ì¸ìš©)
        tokenizer_dir = os.path.join(temp_dir, "tokenizer")
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
        
        # Special tokens ì¶”ê°€ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
        if 'tokenizer' in config and 'special_tokens' in config['tokenizer']:
            special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}
            tokenizer.add_special_tokens(special_tokens_dict)
            log.info(f"Special tokens ì¶”ê°€: {config['tokenizer']['special_tokens']}")
        
        log.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        
        # ì €ì¥ëœ ëª¨ë¸ì˜ config.json ì§ì ‘ ë¡œë“œ ë° ìˆ˜ì •
        try:
            # ì €ì¥ëœ config.json íŒŒì¼ ì½ê¸°
            config_path = os.path.join(temp_dir, "config.json")
            if os.path.exists(config_path):
                with open(config_path, "r", encoding='utf-8') as f:
                    model_config_dict = json.load(f)
                
                # ë¶„ë¥˜ ê´€ë ¨ ì„¤ì • ì œê±°
                model_config_dict.pop('num_labels', None)
                model_config_dict.pop('id2label', None)
                model_config_dict.pop('label2id', None)
                
                # ìˆ˜ì •ëœ configë¡œ BartConfig ìƒì„±
                bart_config = BartConfig(**model_config_dict)
                log.info(f"BART ì„¤ì • ë¡œë“œ ì™„ë£Œ (config.json ì‚¬ìš©), vocab_size: {bart_config.vocab_size}")
            else:
                # config.jsonì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©
                model_name = config['general']['model_name']
                bart_config = BartConfig.from_pretrained(model_name)
                actual_vocab_size = len(tokenizer)
                bart_config.vocab_size = actual_vocab_size
                log.info(f"BART ì„¤ì • ë¡œë“œ ì™„ë£Œ (ê¸°ë³¸ ë°©ì‹), vocab_size: {actual_vocab_size}")
        except Exception as e:
            log.warning(f"config.json ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
            model_name = config['general']['model_name']
            bart_config = BartConfig.from_pretrained(model_name)
            actual_vocab_size = len(tokenizer)
            bart_config.vocab_size = actual_vocab_size
        
        # ëª¨ë¸ ë¡œë“œ (configì˜ vocab_sizeê°€ ì¡°ì •ëœ ìƒíƒœ)
        model = BartForConditionalGeneration.from_pretrained(temp_dir, config=bart_config)
        
        # í† í° ì„ë² ë”© í¬ê¸° ì¡°ì • (í•„ìˆ˜! wandb_sweep.pyì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬)
        # special tokensê°€ ì¶”ê°€ëœ ê²½ìš° ë°˜ë“œì‹œ í•„ìš”
        model.resize_token_embeddings(len(tokenizer))
        model.eval()  # evaluation ëª¨ë“œ ì„¤ì •
        log.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        return model, tokenizer, config, metadata
        
    except Exception as e:
        log.error(f"ëª¨ë¸ íŒ¨í‚¤ì§€ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        log.error(f"ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´: {type(e).__name__}")
        if "num_labels" in str(e) and "id2label" in str(e):
            log.error("íˆíŠ¸: BART ëª¨ë¸ ì„¤ì • ë¶ˆì¼ì¹˜ ë¬¸ì œì…ë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•˜ê±°ë‚˜ config ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        raise
        
    finally:
        # ì„ì‹œ í´ë” ì‚­ì œ
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir, ignore_errors=True)

def evaluate_ensemble_results_with_baseline(predictions, references, config, tokenizer):
    """
    ì•™ìƒë¸” ê²°ê³¼ë¥¼ baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í‰ê°€
    
    Args:
        predictions: ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        references: ì°¸ì¡° ìš”ì•½ ë¦¬ìŠ¤íŠ¸
        config: ì„¤ì •
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        dict: ROUGE ë©”íŠ¸ë¦­ ê²°ê³¼
    """
    log.info("ì•™ìƒë¸” ê²°ê³¼ë¥¼ baseline.py ë°©ì‹ìœ¼ë¡œ í‰ê°€ ì‹œì‘")
    
    # compute_metrics í•¨ìˆ˜ ì§ì ‘ ì‚¬ìš© (baseline.pyì™€ ë™ì¼)
    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ì—¬ compute_metricsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ë³€í™˜
    pred_tokens = []
    label_tokens = []
    
    for pred, ref in zip(predictions, references):
        # ì˜ˆì¸¡ ê²°ê³¼ í† í¬ë‚˜ì´ì§•
        pred_encoded = tokenizer.encode(pred, return_tensors="pt", truncation=True, max_length=512)
        pred_tokens.append(pred_encoded.squeeze().tolist())
        
        # ì°¸ì¡° ê²°ê³¼ í† í¬ë‚˜ì´ì§•
        ref_encoded = tokenizer.encode(ref, return_tensors="pt", truncation=True, max_length=512)
        label_tokens.append(ref_encoded.squeeze().tolist())
    
    # compute_metricsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœë¡œ ë°ì´í„° êµ¬ì„±
    from collections import namedtuple
    import numpy as np
    
    # ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©
    max_pred_len = max(len(tokens) for tokens in pred_tokens)
    max_label_len = max(len(tokens) for tokens in label_tokens)
    
    padded_predictions = []
    padded_labels = []
    
    for tokens in pred_tokens:
        padded = tokens + [tokenizer.pad_token_id] * (max_pred_len - len(tokens))
        padded_predictions.append(padded)
    
    for tokens in label_tokens:
        padded = tokens + [-100] * (max_label_len - len(tokens))  # -100ì€ ì†ì‹¤ ê³„ì‚°ì—ì„œ ë¬´ì‹œë¨
        padded_labels.append(padded)
    
    # compute_metricsê°€ ê¸°ëŒ€í•˜ëŠ” í˜•íƒœì˜ EvalPrediction ê°ì²´ ìƒì„±
    EvalPrediction = namedtuple('EvalPrediction', ['predictions', 'label_ids'])
    eval_pred = EvalPrediction(
        predictions=np.array(padded_predictions),
        label_ids=np.array(padded_labels)
    )
    
    # baseline.pyì˜ compute_metrics í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ
    metrics = compute_metrics(config, tokenizer, eval_pred)
    
    # rouge-avg ì¶”ê°€ (ê°œë³„ ëª¨ë¸ê³¼ ë™ì¼í•œ ë°©ì‹)
    if 'rouge-1' in metrics and 'rouge-2' in metrics and 'rouge-l' in metrics:
        rouge_avg = (metrics['rouge-1'] + metrics['rouge-2'] + metrics['rouge-l']) / 3
        metrics['rouge-avg'] = rouge_avg
    
    log.info("ì•™ìƒë¸” ê²°ê³¼ baseline.py ë°©ì‹ í‰ê°€ ì™„ë£Œ")
    return metrics

def prepare_validation_dataset_for_ensemble(config, preprocessor, tokenizer):
    """
    baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        preprocessor: ë°ì´í„° ì „ì²˜ë¦¬ê¸°
        tokenizer: í† í¬ë‚˜ì´ì €
        
    Returns:
        DatasetForVal: ê²€ì¦ ë°ì´í„°ì…‹
    """
    log.info("ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹œì‘ (baseline.py ë°©ì‹)")
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    data_path = config['general']['data_path']
    val_file_path = os.path.join(data_path, 'dev.csv')
    val_data = preprocessor.make_set_as_df(val_file_path)
    
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    
    # í† í¬ë‚˜ì´ì € ì ìš© (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    val_tokenized_encoder_inputs = tokenizer(
        encoder_input_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['encoder_max_len'], 
        return_token_type_ids=False
    )
    val_tokenized_decoder_inputs = tokenizer(
        decoder_input_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['decoder_max_len'], 
        return_token_type_ids=False
    )
    val_tokenized_decoder_outputs = tokenizer(
        decoder_output_val, 
        return_tensors="pt", 
        padding=True,
        add_special_tokens=True, 
        truncation=True, 
        max_length=config['tokenizer']['decoder_max_len'], 
        return_token_type_ids=False
    )
    
    # baseline.pyì™€ ë™ì¼í•œ DatasetForVal í´ë˜ìŠ¤ ì‚¬ìš©
    val_inputs_dataset = DatasetForVal(
        val_tokenized_encoder_inputs, 
        val_tokenized_decoder_inputs, 
        val_tokenized_decoder_outputs,
        len(encoder_input_val)
    )
    
    log.info("ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
    return val_inputs_dataset

def evaluate_single_model_with_baseline(model, tokenizer, config):
    """
    baseline.py ë°©ì‹ìœ¼ë¡œ ë‹¨ì¼ ëª¨ë¸ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
    
    Args:
        model: ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì •
        
    Returns:
        dict: ROUGE ë©”íŠ¸ë¦­ ê²°ê³¼
    """
    log.info("baseline.py ë°©ì‹ìœ¼ë¡œ ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
    
    # ë°ì´í„° ì „ì²˜ë¦¬ê¸° ìƒì„± (baseline.pyì™€ ë™ì¼)
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])
    
    # ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„ (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    val_inputs_dataset = prepare_validation_dataset_for_ensemble(config, preprocessor, tokenizer)
    
    # Seq2SeqTrainingArguments ì„¤ì • (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°, wandb ë¹„í™œì„±í™”)
    temp_dir = tempfile.mkdtemp()
    training_args = Seq2SeqTrainingArguments(
        output_dir=temp_dir,
        predict_with_generate=config['training']['predict_with_generate'],
        generation_max_length=config['training']['generation_max_length'],
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        seed=config['training']['seed'],
        report_to=[],  # wandb ë¹„í™œì„±í™”
        logging_strategy="no",  # ë¡œê¹… ë¹„í™œì„±í™”
    )
    
    # compute_metrics í•¨ìˆ˜ë¥¼ ìœ„í•œ wrapper (baseline.pyì™€ ë™ì¼)
    def compute_metrics_wrapper(pred):
        return compute_metrics(config, tokenizer, pred)
    
    # Seq2SeqTrainer ìƒì„± (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        eval_dataset=val_inputs_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics_wrapper
    )
    
    # í‰ê°€ ìˆ˜í–‰ (baseline.pyì™€ ì™„ì „íˆ ë™ì¼í•œ ë°©ì‹)
    log.info("Seq2SeqTrainerë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹œì‘")
    eval_results = trainer.evaluate()
    log.info("í‰ê°€ ì™„ë£Œ")
    
    # ê²°ê³¼ ì¶”ì¶œ
    rouge_results = {}
    for key, value in eval_results.items():
        if 'rouge' in key and key != 'eval_rouge_avg':
            metric_name = key.replace('eval_', '')
            rouge_results[metric_name] = value
    
    # rouge-avg ê³„ì‚° ì¶”ê°€
    if 'rouge-1' in rouge_results and 'rouge-2' in rouge_results and 'rouge-l' in rouge_results:
        rouge_avg = (rouge_results['rouge-1'] + rouge_results['rouge-2'] + rouge_results['rouge-l']) / 3
        rouge_results['rouge-avg'] = rouge_avg
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir, ignore_errors=True)
    
    return rouge_results

class RealtimeTokenEnsemble:
    """
    ì‹¤ì‹œê°„ í† í° ë‹¨ìœ„ ì•™ìƒë¸” í´ë˜ìŠ¤
    ê° ìŠ¤í…ë§ˆë‹¤ ëª¨ë“  ëª¨ë¸ì—ì„œ ë‹¤ìŒ í† í° í™•ë¥  ë¶„í¬ë¥¼ íšë“í•˜ì—¬ ì•™ìƒë¸”
    """
    
    def __init__(self, model_paths, device="cuda:0"):
        """
        Args:
            model_paths: ëª¨ë¸ ZIP íŒŒì¼ ê²½ë¡œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.model_paths = model_paths
        self.device = device
        self.models = []
        self.tokenizers = []
        self.configs = []
        self.metadata_list = []
        
        log.info(f"ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {len(model_paths)}ê°œ ëª¨ë¸")
        log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ë“¤ì„ ë¡œë”©"""
        log.info("ëª¨ë¸ë“¤ ë¡œë”© ì‹œì‘...")
        
        for i, path in enumerate(self.model_paths):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.model_paths)} ë¡œë”© ì¤‘: {path}")
            
            try:
                model, tokenizer, config, metadata = load_model_package(path)
                
                # GPU ë©”ëª¨ë¦¬ í™•ì¸ ë° ëª¨ë¸ ë¡œë”©
                try:
                    model.to(self.device)
                    model.eval()
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        log.error(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨. CPUë¡œ fallback ì‹œë„...")
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        self.device = "cpu"
                        model.to(self.device)
                        model.eval()
                        log.warning(f"ëª¨ë¸ {i+1}ì„ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì„±ëŠ¥ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        raise
                
                self.models.append(model)
                self.tokenizers.append(tokenizer)
                self.configs.append(config)
                self.metadata_list.append(metadata)
                
                log.info(f"ëª¨ë¸ {i+1} ë¡œë”© ì™„ë£Œ: {metadata.get('wandb_run_name', 'Unknown')} (device: {self.device})")
                
            except Exception as e:
                log.error(f"ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨: {e}")
                log.error(f"ê²½ë¡œ: {path}")
                raise
        
        log.info(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def generate_ensemble_sequence_single(self, input_text, config):
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” (ê°œì„ ëœ ë¡œì§)
        
        Args:
            input_text: ë‹¨ì¼ ì…ë ¥ í…ìŠ¤íŠ¸
            config: ìƒì„± ì„¤ì •
            
        Returns:
            str: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        tokenizer = self.tokenizers[0]
        max_length = config['inference']['generate_max_length']
        
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            max_length=config['tokenizer']['encoder_max_len'],
            truncation=True,
            padding=True
        ).to(self.device)
        
        # ğŸš€ í•µì‹¬ ìµœì í™”: ê° ëª¨ë¸ì˜ encoder outputì„ í•œ ë²ˆë§Œ ê³„ì‚°
        model_encoder_outputs = []
        for model in self.models:
            with torch.no_grad():
                encoder_outputs = model.get_encoder()(
                    input_ids=inputs['input_ids'], 
                    attention_mask=inputs['attention_mask']
                )
                model_encoder_outputs.append(encoder_outputs.last_hidden_state)
        
        # ë””ì½”ë” ì‹œì‘ í† í°
        decoder_start_token_id = tokenizer.bos_token_id
        if decoder_start_token_id is None:
            decoder_start_token_id = tokenizer.eos_token_id
        
        # ìƒì„±ëœ ì‹œí€€ìŠ¤ (ì‹œì‘ í† í°ìœ¼ë¡œ ì´ˆê¸°í™”)
        generated_sequence = [decoder_start_token_id]
        eos_token_id = tokenizer.eos_token_id
        
        # ğŸ”„ í† í°ë³„ ìƒì„± ë£¨í”„
        for step in range(max_length - 1):
            # í˜„ì¬ê¹Œì§€ì˜ ì‹œí€€ìŠ¤ë¥¼ í…ì„œë¡œ ë³€í™˜
            current_ids = torch.tensor([generated_sequence], device=self.device)
            
            # ê° ëª¨ë¸ì—ì„œ ë‹¤ìŒ í† í° logits ê³„ì‚°
            model_logits = []
            successful_models = 0
            
            for i, model in enumerate(self.models):
                try:
                    with torch.no_grad():
                        # ë””ì½”ë” ì‹¤í–‰ (ë¯¸ë¦¬ ê³„ì‚°ëœ encoder output ì‚¬ìš©)
                        decoder_outputs = model.get_decoder()(
                            input_ids=current_ids,
                            encoder_hidden_states=model_encoder_outputs[i],
                            encoder_attention_mask=inputs['attention_mask']
                        )
                        
                        # LM headë¡œ vocabulary logits ê³„ì‚°
                        logits = model.lm_head(decoder_outputs.last_hidden_state)
                        next_token_logits = logits[0, -1, :]  # ë§ˆì§€ë§‰ ìœ„ì¹˜ì˜ logits
                        
                        model_logits.append(next_token_logits)
                        successful_models += 1
                        
                except Exception as e:
                    log.warning(f"ëª¨ë¸ {i+1} ìŠ¤í… {step} ì˜¤ë¥˜: {e}")
                    continue
            
            if successful_models == 0:
                log.error(f"ìŠ¤í… {step}: ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨")
                break
            
            # ğŸ§® ì„±ê³µí•œ ëª¨ë¸ë“¤ì˜ logits í‰ê·  ê³„ì‚°
            if len(model_logits) > 1:
                ensemble_logits = torch.stack(model_logits).mean(dim=0)
            else:
                ensemble_logits = model_logits[0]
            
            # ğŸ¯ Greedy decoding: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
            next_token_id = torch.argmax(ensemble_logits).item()
            
            # ìƒì„±ëœ í† í°ì„ ì‹œí€€ìŠ¤ì— ì¶”ê°€
            generated_sequence.append(next_token_id)
            
            # âœ… EOS í† í° ë„ë‹¬ ì‹œ ìƒì„± ì¢…ë£Œ
            if next_token_id == eos_token_id:
                log.debug(f"EOS ë„ë‹¬: ìŠ¤í… {step}, ê¸¸ì´ {len(generated_sequence)}")
                break
        
        # ğŸ”¤ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (baseline.pyì™€ ë™ì¼í•˜ê²Œ íŠ¹ìˆ˜ í† í° ìœ ì§€)
        generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)
        
        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
        for token in config['inference']['remove_tokens']:
            generated_text = generated_text.replace(token, " ")
            
        return generated_text.strip()
    
    def generate_ensemble_sequence(self, input_ids, config):
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼ (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            input_ids: ì…ë ¥ í† í° ID
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬  
            config: ìƒì„± ì„¤ì •
            
        Returns:
            torch.Tensor: ìƒì„±ëœ ì‹œí€€ìŠ¤
        """
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ìœ„ì„
        tokenizer = self.tokenizers[0]
        input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)
        generated_text = self.generate_ensemble_sequence_single(input_text, config)
        
        # ë‹¤ì‹œ í† í°í™”í•˜ì—¬ ë°˜í™˜
        generated_ids = tokenizer(
            generated_text, 
            return_tensors="pt",
            add_special_tokens=False
        )['input_ids']
        
        return generated_ids
    
    def generate_with_realtime_ensemble(self, input_texts, config):
        """
        ì‹¤ì‹œê°„ ì•™ìƒë¸”ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ìƒì„± ì„¤ì •
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        log.info(f"Realtime Token Ensemble ì‹œì‘: {len(input_texts)}ê°œ í…ìŠ¤íŠ¸")
        
        for i, text in enumerate(tqdm(input_texts, desc="ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘")):
            try:
                # ğŸš€ ê°œì„ ëœ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‚¬ìš©
                generated_text = self.generate_ensemble_sequence_single(text, config)
                results.append(generated_text)
                
                # ì§„í–‰ ìƒí™© ë¡œê¹… (ë§¤ 10ê°œë§ˆë‹¤)
                if (i + 1) % 10 == 0:
                    log.info(f"ì§„í–‰ ìƒí™©: {i+1}/{len(input_texts)} ì™„ë£Œ")
                
            except Exception as e:
                log.warning(f"í…ìŠ¤íŠ¸ {i+1} ì‹¤ì‹œê°„ ì•™ìƒë¸” ì˜¤ë¥˜: {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("Realtime Token Ensemble ì™„ë£Œ")
        return results
    
    def generate_with_single_model(self, model, tokenizer, config, input_texts):
        """
        ë¹„êµë¥¼ ìœ„í•œ ë‹¨ì¼ ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €  
            config: ì„¤ì •
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in tqdm(input_texts, desc="ë‹¨ì¼ ëª¨ë¸ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘"):
            try:
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)
                
                # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                for token in config['inference']['remove_tokens']:
                    generated_text = generated_text.replace(token, " ")
                
                results.append(generated_text.strip())
                
            except Exception as e:
                log.warning(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        return results
    
    def generate_token_ids_with_single_model(self, model, tokenizer, config, input_texts):
        """
        ë‹¨ì¼ ëª¨ë¸ë¡œ í† í° ID ìƒì„± (í† í° ë ˆë²¨ ì•™ìƒë¸”ì„ ìœ„í•¨)
        
        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €  
            config: ì„¤ì •
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ìƒì„±ëœ í† í° ID í…ì„œ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in tqdm(input_texts, desc="ë‹¨ì¼ ëª¨ë¸ í† í° ID ìƒì„± ì¤‘"):
            try:
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                # í† í° IDë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                results.append(generated_ids[0].cpu())
                
            except Exception as e:
                log.warning(f"í† í° ID ìƒì„± ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                # ë¹ˆ í† í° ì‹œí€€ìŠ¤ ìƒì„± (pad_token_idë§Œ í¬í•¨)
                fallback_ids = torch.tensor([tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id])
                results.append(fallback_ids)
        
        return results
    
    def evaluate_on_validation(self, val_data_path):
        """
        ì‹¤ì‹œê°„ ì•™ìƒë¸” ê²€ì¦ ë°ì´í„° í‰ê°€
        
        Args:
            val_data_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼
        """
        import time
        
        log.info(f"Realtime Token Ensemble ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘: {val_data_path}")
        
        # ê²€ì¦ ë°ì´í„° ë¡œë“œ
        try:
            val_df = pd.read_csv(val_data_path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            required_columns = ['dialogue', 'summary']
            for col in required_columns:
                if col not in val_df.columns:
                    log.error(f"ê²€ì¦ ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ '{col}'ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(val_df.columns)}")
                    return None
            
            val_df_sample = val_df.head(50)  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 50ê°œë§Œ
            input_texts = val_df_sample['dialogue'].tolist()
            reference_summaries = val_df_sample['summary'].tolist()
            
            # ë¹ˆ ë°ì´í„° í™•ì¸
            if not input_texts or not reference_summaries:
                log.error("ê²€ì¦ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
                
            log.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(input_texts)}ê°œ ìƒ˜í”Œ")
        except FileNotFoundError:
            log.error(f"ê²€ì¦ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_data_path}")
            return None
        except pd.errors.EmptyDataError:
            log.error(f"ê²€ì¦ ë°ì´í„° íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {val_data_path}")
            return None
        except Exception as e:
            log.error(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        # Realtime Token Ensembleìœ¼ë¡œ ìƒì„±
        log.info("Realtime Token Ensemble ìƒì„± ì‹œì‘...")
        realtime_results = self.generate_with_realtime_ensemble(input_texts, self.configs[0])
        
        generation_time = time.time() - start_time
        log.info(f"Realtime Token Ensemble ìƒì„± ì™„ë£Œ: {generation_time:.2f}ì´ˆ")
        
        # ROUGE ì ìˆ˜ ê³„ì‚° (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
        def calculate_rouge_scores(predictions, references, method_name):
            from rouge import Rouge
            rouge = Rouge()
            
            # baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í† í° ì œê±° (ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´)
            replaced_predictions = predictions.copy()
            replaced_references = references.copy()
            remove_tokens = self.configs[0]['inference']['remove_tokens']
            for token in remove_tokens:
                replaced_predictions = [sentence.replace(token, " ") for sentence in replaced_predictions]
                replaced_references = [sentence.replace(token, " ") for sentence in replaced_references]
            
            # baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”
            cleaned_predictions = []
            cleaned_references = []
            for pred, ref in zip(replaced_predictions, replaced_references):
                # ê³µë°± ì •ë¦¬ (baseline.pyì˜ clean_up_tokenization_spaces=True íš¨ê³¼ ëª¨ë°©)
                pred_clean = " ".join(pred.split()).strip()
                ref_clean = " ".join(ref.split()).strip()
                    
                cleaned_predictions.append(pred_clean)
                cleaned_references.append(ref_clean)
            
            try:
                # ë¹ˆ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ rouge ê³„ì‚° ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
                final_predictions = []
                final_references = []
                for pred, ref in zip(cleaned_predictions, cleaned_references):
                    if pred.strip() and ref.strip():
                        final_predictions.append(pred)
                        final_references.append(ref)
                    else:
                        # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° "empty"ë¡œ ëŒ€ì²´
                        final_predictions.append("empty" if not pred.strip() else pred)
                        final_references.append("empty" if not ref.strip() else ref)
                
                rouge_results = rouge.get_scores(final_predictions, final_references, avg=True)
                rouge_scores = {key: value["f"] for key, value in rouge_results.items()}
                # rouge-avg ê³„ì‚° ì¶”ê°€
                rouge_avg = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
                rouge_scores['rouge-avg'] = rouge_avg
                
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-1: {rouge_scores['rouge-1']:.4f}, "
                        f"ROUGE-2: {rouge_scores['rouge-2']:.4f}, ROUGE-L: {rouge_scores['rouge-l']:.4f}, "
                        f"ROUGE-avg: {rouge_scores['rouge-avg']:.4f}")
                return rouge_scores
            except Exception as e:
                log.warning(f"{method_name} ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
                return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'rouge-avg': 0.0}
        
        realtime_scores = calculate_rouge_scores(realtime_results, reference_summaries, "Realtime Token Ensemble")
        
        evaluation_results = {
            'realtime_token_ensemble_scores': realtime_scores,
            'generation_time_seconds': generation_time,
            'num_validation_samples': len(input_texts)
        }
        
        log.info("Realtime Token Ensemble ê²€ì¦ ë°ì´í„° í‰ê°€ ì™„ë£Œ")
        return evaluation_results
    
    def run_ensemble(self, test_data_path):
        """
        Realtime Token Ensemble ì‹¤í–‰
        
        Args:
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            tuple: (ensemble_result_df, generation_time)
        """
        import time
        
        log.info(f"Realtime Token Ensemble ì¶”ë¡  ì‹œì‘: {test_data_path}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        try:
            test_df = pd.read_csv(test_data_path)
            test_df_sample = test_df.head(200)  # 200ê°œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
            input_texts = test_df_sample['dialogue'].tolist()
            log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(input_texts)}ê°œ ìƒ˜í”Œ")
        except Exception as e:
            log.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, 0
        
        # ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        
        # Realtime Token Ensemble ì‹¤í–‰
        realtime_results = self.generate_with_realtime_ensemble(input_texts, self.configs[0])
        
        generation_time = time.time() - start_time
        log.info(f"Realtime Token Ensemble ì™„ë£Œ: {generation_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        realtime_df = pd.DataFrame({
            'fname': test_df_sample['fname'],
            'summary': realtime_results
        })
        
        return realtime_df, generation_time

def main_comprehensive_experiment():
    """
    ğŸ”¬ ë‹¤ì„¯ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ì¢…í•© ë¹„êµ ì‹¤í—˜
    
    1. í•˜ë“œ ë³´íŒ… (Token-level Hard Voting)
    2. ì†Œí”„íŠ¸ ë³´íŒ… (Probability-based Soft Voting) 
    3. ê¸¸ì´ ê¸°ë°˜ (Length-based Selection)
    4. Logit ë ˆë²¨ ì•™ìƒë¸” (Logit-level Ensemble)
    5. ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” (Realtime Token Ensemble)
    """
    import time
    
    log.info("ğŸ”¬ " + "="*60)
    log.info("ğŸ¯ ë‹¤ì„¯ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ì¢…í•© ë¹„êµ ì‹¤í—˜ ì‹œì‘")
    log.info("="*60)
    
    # ê³µí†µ í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    existing_model_paths = get_model_paths()
    if not existing_model_paths:
        log.error("ğŸ’¥ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    log.info(f"ğŸš€ ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ ì‹¤í—˜ ì§„í–‰")
    
    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    val_data_path = "../../input/data/dev.csv"
    test_data_path = "../../input/data/test.csv"
    
    if not os.path.exists(val_data_path):
        log.error(f"ğŸ’¥ ê²€ì¦ ë°ì´í„° ì—†ìŒ: {val_data_path}")
        return
    if not os.path.exists(test_data_path):
        log.error(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ: {test_data_path}")
        return
    
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ì‹¤í—˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    experiment_results = {
        'timestamp': timestamp,
        'model_paths': existing_model_paths,
        'device': device,
        'methods': {},
        'performance_ranking': [],
        'time_ranking': []
    }
    
    # ğŸ“Š ì‹¤í—˜ 1: HardVotingEnsembleì˜ ì„¸ ê°€ì§€ ë°©ì‹
    log.info("\n" + "ğŸ”¥ " + "="*50)
    log.info("ğŸ“Š ì‹¤í—˜ 1: Post-processing ì•™ìƒë¸” ë°©ì‹ë“¤")
    log.info("="*50)
    
    hard_ensemble = PostProcessingEnsemble(existing_model_paths, device=device)
    hard_ensemble.load_models()
    
    # ê²€ì¦ ë°ì´í„° í‰ê°€
    start_time = time.time()
    hard_evaluation = hard_ensemble.evaluate_on_validation(val_data_path)
    hard_time = time.time() - start_time
    
    if hard_evaluation:
        experiment_results['methods']['hard_voting'] = {
            'rouge_scores': hard_evaluation['hard_voting_scores'],
            'time_seconds': hard_time,
            'method_type': 'Post-processing'
        }
        experiment_results['methods']['soft_voting'] = {
            'rouge_scores': hard_evaluation['soft_voting_scores'],
            'time_seconds': hard_time,  # ê°™ì€ ì‹¤í–‰ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼
            'method_type': 'Post-processing'
        }
        experiment_results['methods']['length_based'] = {
            'rouge_scores': hard_evaluation['length_based_scores'],
            'time_seconds': hard_time,  # ê°™ì€ ì‹¤í–‰ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼  
            'method_type': 'Post-processing'
        }
        experiment_results['methods']['logit_level'] = {
            'rouge_scores': hard_evaluation['logit_level_scores'],
            'time_seconds': hard_time,  # ê°™ì€ ì‹¤í–‰ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼
            'method_type': 'Post-processing'
        }
    
    # ğŸ“Š ì‹¤í—˜ 2: RealtimeTokenEnsemble
    log.info("\n" + "ğŸ”¥ " + "="*50)
    log.info("âš¡ ì‹¤í—˜ 2: ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”")
    log.info("="*50)
    
    try:
        realtime_ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
        realtime_ensemble.load_models()
        
        # ê²€ì¦ ë°ì´í„° í‰ê°€
        start_time = time.time()
        realtime_evaluation = realtime_ensemble.evaluate_on_validation(val_data_path)
        realtime_time = time.time() - start_time
        
        if realtime_evaluation:
            experiment_results['methods']['realtime_token_ensemble'] = {
                'rouge_scores': realtime_evaluation['realtime_token_ensemble_scores'],
                'time_seconds': realtime_time,
                'method_type': 'Runtime'
            }
    except Exception as e:
        log.error(f"ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        experiment_results['methods']['realtime_token_ensemble'] = {
            'error': str(e),
            'method_type': 'Runtime'
        }
    
    # ğŸ“ˆ ì„±ëŠ¥ ìˆœìœ„ ë¶„ì„
    log.info("\n" + "ğŸ† " + "="*50)
    log.info("ğŸ“ˆ ì¢…í•© ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼")
    log.info("="*50)
    
    # ROUGE-avg ê¸°ì¤€ ì„±ëŠ¥ ìˆœìœ„
    performance_data = []
    for method_name, method_data in experiment_results['methods'].items():
        if 'rouge_scores' in method_data:
            rouge_avg = method_data['rouge_scores']['rouge-avg']
            time_taken = method_data['time_seconds']
            performance_data.append((method_name, rouge_avg, time_taken))
    
    if performance_data:
        # ì„±ëŠ¥ìˆœ ì •ë ¬
        performance_data.sort(key=lambda x: x[1], reverse=True)
        experiment_results['performance_ranking'] = performance_data
        
        # ì†ë„ìˆœ ì •ë ¬
        time_data = sorted(performance_data, key=lambda x: x[2])
        experiment_results['time_ranking'] = time_data
        
        log.info("ğŸ¥‡ ì„±ëŠ¥ ìˆœìœ„ (ROUGE-avg ê¸°ì¤€):")
        for i, (method, rouge_avg, time_taken) in enumerate(performance_data, 1):
            method_type = experiment_results['methods'][method]['method_type']
            log.info(f"  {i}ìœ„. {method}: {rouge_avg:.4f} ({time_taken:.1f}ì´ˆ, {method_type})")
        
        log.info("\nâš¡ ì†ë„ ìˆœìœ„:")
        for i, (method, rouge_avg, time_taken) in enumerate(time_data, 1):
            method_type = experiment_results['methods'][method]['method_type']
            log.info(f"  {i}ìœ„. {method}: {time_taken:.1f}ì´ˆ (ROUGE-avg: {rouge_avg:.4f})")
        
        # ğŸ“Š ìƒì„¸ ì ìˆ˜ ì¶œë ¥
        log.info("\nğŸ“Š ìƒì„¸ ROUGE ì ìˆ˜:")
        for method_name, method_data in experiment_results['methods'].items():
            if 'rouge_scores' in method_data:
                scores = method_data['rouge_scores']
                log.info(f"\nğŸ”¹ {method_name}:")
                log.info(f"   ROUGE-1: {scores['rouge-1']:.4f}")
                log.info(f"   ROUGE-2: {scores['rouge-2']:.4f}")
                log.info(f"   ROUGE-L: {scores['rouge-l']:.4f}")
                log.info(f"   ROUGE-avg: {scores['rouge-avg']:.4f}")
                log.info(f"   ì‹¤í–‰ì‹œê°„: {method_data['time_seconds']:.1f}ì´ˆ")
        
        # ğŸ¯ ìµœì  ë°©ì‹ ì¶”ì²œ
        best_performance = performance_data[0]
        fastest_method = time_data[0]
        
        log.info("\n" + "ğŸ¯ " + "="*50)
        log.info("ğŸ’¡ ì¶”ì²œ ê²°ê³¼")
        log.info("="*50)
        log.info(f"ğŸ† ìµœê³  ì„±ëŠ¥: {best_performance[0]} (ROUGE-avg: {best_performance[1]:.4f})")
        log.info(f"âš¡ ìµœê³  ì†ë„: {fastest_method[0]} ({fastest_method[2]:.1f}ì´ˆ)")
        
        # ì„±ëŠ¥ vs ì†ë„ trade-off ë¶„ì„
        performance_gap = best_performance[1] - fastest_method[1] 
        speed_ratio = fastest_method[2] / best_performance[2]
        
        if performance_gap < 0.01 and speed_ratio < 0.5:
            log.info(f"ğŸ’ ì¶”ì²œ: {fastest_method[0]} (ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸í•˜ê³  ì†ë„ ìš°ìˆ˜)")
        elif performance_gap > 0.02:
            log.info(f"ğŸ’ ì¶”ì²œ: {best_performance[0]} (ì„±ëŠ¥ ì°¨ì´ ìœ ì˜ë¯¸)")
        else:
            log.info("ğŸ’­ ì„±ëŠ¥ê³¼ ì†ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ìš©ë„ì— ë§ê²Œ ì„ íƒí•˜ì„¸ìš”")
    
    # ğŸ“ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° CSV ì €ì¥
    log.info("\n" + "ğŸ’¾ " + "="*50)
    log.info("ğŸ“¤ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° CSV ì €ì¥ ì‹œì‘")
    log.info("="*50)
    
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = "./ensemble_results"
    os.makedirs(results_dir, exist_ok=True)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ í™•ì¸
    if os.path.exists(test_data_path):
        # PostProcessingEnsembleë¡œ 3ê°€ì§€ ë°©ì‹ ì¶”ë¡ 
        if hard_ensemble:
            log.info("ğŸ“Š í›„ì²˜ë¦¬ ì•™ìƒë¸” ë°©ì‹ë“¤ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì¤‘...")
            ensemble_results_dict, _ = hard_ensemble.run_ensemble(test_data_path)
            
            # ê° ë°©ì‹ë³„ CSV ì €ì¥
            hard_voting_path = os.path.join(results_dir, f"ensemble_hard_voting_{timestamp}.csv")
            ensemble_results_dict['hard_voting'].to_csv(hard_voting_path, index=False, encoding='utf-8')
            log.info(f"ğŸ’¾ í•˜ë“œ ë³´íŒ… ê²°ê³¼ ì €ì¥: {hard_voting_path}")
            
            soft_voting_path = os.path.join(results_dir, f"ensemble_soft_voting_{timestamp}.csv")
            ensemble_results_dict['soft_voting'].to_csv(soft_voting_path, index=False, encoding='utf-8')
            log.info(f"ğŸ’¾ ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼ ì €ì¥: {soft_voting_path}")
            
            length_based_path = os.path.join(results_dir, f"ensemble_length_based_{timestamp}.csv")
            ensemble_results_dict['length_based'].to_csv(length_based_path, index=False, encoding='utf-8')
            log.info(f"ğŸ’¾ ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼ ì €ì¥: {length_based_path}")
            
            logit_level_path = os.path.join(results_dir, f"ensemble_logit_level_{timestamp}.csv")
            ensemble_results_dict['logit_level'].to_csv(logit_level_path, index=False, encoding='utf-8')
            log.info(f"ğŸ’¾ Logit ë ˆë²¨ ê²°ê³¼ ì €ì¥: {logit_level_path}")
        
        # RealtimeTokenEnsembleë¡œ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ì¶”ë¡ 
        try:
            if 'realtime_token_ensemble' in experiment_results['methods'] and 'error' not in experiment_results['methods']['realtime_token_ensemble']:
                log.info("âš¡ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ì¤‘...")
                realtime_ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
                realtime_ensemble.load_models()
                
                realtime_df, _ = realtime_ensemble.run_ensemble(test_data_path)
                realtime_path = os.path.join(results_dir, f"ensemble_realtime_token_{timestamp}.csv")
                realtime_df.to_csv(realtime_path, index=False, encoding='utf-8')
                log.info(f"ğŸ’¾ ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {realtime_path}")
        except Exception as e:
            log.warning(f"ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        log.warning(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ CSV ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤: {test_data_path}")
    
    # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥
    experiment_metadata_path = os.path.join(results_dir, f"comprehensive_experiment_{timestamp}.json")
    with open(experiment_metadata_path, "w", encoding='utf-8') as f:
        json.dump(experiment_results, f, indent=2, ensure_ascii=False)
    log.info(f"\nğŸ’¾ ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥: {experiment_metadata_path}")
    
    log.info("\n" + "ğŸ‰ " + "="*50)
    log.info("âœ… ì¢…í•© ë¹„êµ ì‹¤í—˜ ì™„ë£Œ!")
    log.info("="*50)
    
    return experiment_results

class PostProcessingEnsemble:
    """
    í›„ì²˜ë¦¬ ê¸°ë°˜ ì•™ìƒë¸” ì¶”ë¡  í´ë˜ìŠ¤
    ê° ëª¨ë¸ì´ ë…ë¦½ì ìœ¼ë¡œ ì™„ì „í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì•™ìƒë¸”:
    - í•˜ë“œ ë³´íŒ…: í† í° ë‹¨ìœ„ ë‹¤ìˆ˜ê²°
    - ì†Œí”„íŠ¸ ë³´íŒ…: í™•ë¥  ë¶„í¬ í‰ê· 
    - ê¸¸ì´ ê¸°ë°˜: ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
    """
    
    def __init__(self, model_paths, device="cuda:0"):
        """
        Args:
            model_paths: ëª¨ë¸ ZIP íŒŒì¼ ê²½ë¡œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.model_paths = model_paths
        self.device = device
        self.models = []
        self.tokenizers = []
        self.configs = []
        self.metadata_list = []
        
        log.info(f"ì•™ìƒë¸” ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {len(model_paths)}ê°œ ëª¨ë¸")
        log.info(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    def load_models(self):
        """ëª¨ë“  ëª¨ë¸ë“¤ì„ ë¡œë”©"""
        log.info("ëª¨ë¸ë“¤ ë¡œë”© ì‹œì‘...")
        
        for i, path in enumerate(self.model_paths):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.model_paths)} ë¡œë”© ì¤‘: {path}")
            
            try:
                model, tokenizer, config, metadata = load_model_package(path)
                
                # GPU ë©”ëª¨ë¦¬ í™•ì¸ ë° ëª¨ë¸ ë¡œë”©
                try:
                    model.to(self.device)
                    model.eval()
                except RuntimeError as e:
                    if "out of memory" in str(e).lower():
                        log.error(f"GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨. CPUë¡œ fallback ì‹œë„...")
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        self.device = "cpu"
                        model.to(self.device)
                        model.eval()
                        log.warning(f"ëª¨ë¸ {i+1}ì„ CPUì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì„±ëŠ¥ì´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        raise
                
                self.models.append(model)
                self.tokenizers.append(tokenizer)
                self.configs.append(config)
                self.metadata_list.append(metadata)
                
                log.info(f"ëª¨ë¸ {i+1} ë¡œë”© ì™„ë£Œ: {metadata.get('wandb_run_name', 'Unknown')} (device: {self.device})")
                
            except Exception as e:
                log.error(f"ëª¨ë¸ {i+1} ë¡œë”© ì‹¤íŒ¨: {e}")
                log.error(f"ê²½ë¡œ: {path}")
                raise
        
        log.info(f"ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def generate_with_single_model(self, model, tokenizer, config, input_texts):
        """
        ë‹¨ì¼ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €  
            config: ì„¤ì •
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in tqdm(input_texts, desc="í…ìŠ¤íŠ¸ ìƒì„± ì¤‘"):
            try:
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)
                
                # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                for token in config['inference']['remove_tokens']:
                    generated_text = generated_text.replace(token, " ")
                
                results.append(generated_text.strip())
                
            except Exception as e:
                log.warning(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        return results
    
    def generate_token_ids_with_single_model(self, model, tokenizer, config, input_texts):
        """
        ë‹¨ì¼ ëª¨ë¸ë¡œ í† í° ID ìƒì„± (í† í° ë ˆë²¨ ì•™ìƒë¸”ì„ ìœ„í•¨)
        
        Args:
            model: ëª¨ë¸
            tokenizer: í† í¬ë‚˜ì´ì €  
            config: ì„¤ì •
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: ìƒì„±ëœ í† í° ID í…ì„œ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for text in tqdm(input_texts, desc="í† í° ID ìƒì„± ì¤‘"):
            try:
                inputs = tokenizer(
                    text, 
                    return_tensors="pt", 
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_length=config['inference']['generate_max_length'],
                        num_beams=config['inference']['num_beams'],
                        no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                        early_stopping=config['inference']['early_stopping']
                    )
                
                # í† í° IDë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                results.append(generated_ids[0].cpu())
                
            except Exception as e:
                log.warning(f"í† í° ID ìƒì„± ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                # ë¹ˆ í† í° ì‹œí€€ìŠ¤ ìƒì„± (pad_token_idë§Œ í¬í•¨)
                fallback_ids = torch.tensor([tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id])
                results.append(fallback_ids)
        
        return results
    
    def token_id_level_hard_voting(self, token_ids_list, reference_tokenizer):
        """
        í† í° ID ë ˆë²¨ì—ì„œ ì§„ì§œ í•˜ë“œ ë³´íŒ… ìˆ˜í–‰
        
        Args:
            token_ids_list: ê° ëª¨ë¸ë³„ í† í° ID í…ì„œ ë¦¬ìŠ¤íŠ¸ë“¤ [model1_results, model2_results, ...]
            reference_tokenizer: ê¸°ì¤€ í† í¬ë‚˜ì´ì €
            
        Returns:
            list: ì•™ìƒë¸”ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        import torch
        from collections import Counter
        
        ensemble_results = []
        num_samples = len(token_ids_list[0])
        
        log.info("í† í° ID ë ˆë²¨ í•˜ë“œ ë³´íŒ… ì‹œì‘...")
        
        for i in tqdm(range(num_samples), desc="í† í° ID ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            # ê° ìƒ˜í”Œì— ëŒ€í•œ ëª¨ë“  ëª¨ë¸ì˜ í† í° ID ìˆ˜ì§‘
            sample_token_ids = [model_results[i] for model_results in token_ids_list]
            
            # ë¹ˆ í…ì„œ ì œê±°
            valid_token_ids = [ids for ids in sample_token_ids if ids.numel() > 0]
            
            if not valid_token_ids:
                ensemble_results.append("")
                continue
            
            # ìµœëŒ€ ê¸¸ì´ ê²°ì •
            max_len = max(len(ids) for ids in valid_token_ids)
            
            # ê° ìœ„ì¹˜ë³„ë¡œ í† í° ID ë‹¤ìˆ˜ê²°
            ensemble_ids = []
            for pos in range(max_len):
                position_tokens = []
                for ids in valid_token_ids:
                    if pos < len(ids):
                        token_id = ids[pos].item()
                        # íŒ¨ë”© í† í°ì´ë‚˜ íŠ¹ìˆ˜ í† í° ì œì™¸
                        if token_id not in [reference_tokenizer.pad_token_id, reference_tokenizer.eos_token_id]:
                            position_tokens.append(token_id)
                
                if position_tokens:
                    # ë‹¤ìˆ˜ê²°ë¡œ í† í° ì„ íƒ
                    counter = Counter(position_tokens)
                    most_common_token = counter.most_common(1)[0][0]
                    ensemble_ids.append(most_common_token)
                else:
                    # ëª¨ë“  ëª¨ë¸ì´ íŒ¨ë”©ì´ë‚˜ ì¢…ë£Œ í† í°ì„ ì„ íƒí•œ ê²½ìš° ì¢…ë£Œ
                    break
            
            # í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            if ensemble_ids:
                try:
                    ensemble_tensor = torch.tensor(ensemble_ids)
                    generated_text = reference_tokenizer.decode(ensemble_tensor, skip_special_tokens=False)
                    
                    # ë¶ˆí•„ìš”í•œ í† í° ì œê±° (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹)
                    config = self.configs[0]  # ì²« ë²ˆì§¸ ì„¤ì • ì‚¬ìš©
                    for token in config['inference']['remove_tokens']:
                        generated_text = generated_text.replace(token, " ")
                    
                    ensemble_results.append(generated_text.strip())
                except Exception as e:
                    log.warning(f"í† í° ë””ì½”ë”© ì‹¤íŒ¨: {e}")
                    ensemble_results.append("")
            else:
                ensemble_results.append("")
        
        log.info("í† í° ID ë ˆë²¨ í•˜ë“œ ë³´íŒ… ì™„ë£Œ")
        return ensemble_results
    
    def token_level_hard_voting(self, generated_texts_list, reference_tokenizer):
        """
        í† í° ë‹¨ìœ„ í•˜ë“œ ë³´íŒ…
        
        Args:
            generated_texts_list: ê° ëª¨ë¸ë³„ ìƒì„± í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë“¤
            reference_tokenizer: ê¸°ì¤€ í† í¬ë‚˜ì´ì €
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        ensemble_results = []
        num_samples = len(generated_texts_list[0])
        
        log.info("í† í° ë‹¨ìœ„ í•˜ë“œ ë³´íŒ… ì‹œì‘...")
        
        for i in tqdm(range(num_samples), desc="ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            # ê° ìƒ˜í”Œì— ëŒ€í•œ ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜ì§‘
            texts_for_sample = [texts[i] for texts in generated_texts_list]
            
            # ë¹ˆ ë¬¸ìì—´ ì œê±°
            texts_for_sample = [text for text in texts_for_sample if text.strip()]
            
            if not texts_for_sample:
                ensemble_results.append("")
                continue
            
            # í† í°í™”
            tokenized_texts = []
            for text in texts_for_sample:
                try:
                    tokens = reference_tokenizer.tokenize(text)
                    tokenized_texts.append(tokens)
                except:
                    # í† í°í™” ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    tokenized_texts.append([])
            
            # ë¹ˆ í† í° ë¦¬ìŠ¤íŠ¸ ì œê±°
            tokenized_texts = [tokens for tokens in tokenized_texts if tokens]
            
            if not tokenized_texts:
                ensemble_results.append("")
                continue
            
            # ìµœëŒ€ ê¸¸ì´ì— ë§ì¶° ì •ë ¬
            max_len = max(len(tokens) for tokens in tokenized_texts)
            
            # ê° ìœ„ì¹˜ë³„ë¡œ ë‹¤ìˆ˜ê²°
            final_tokens = []
            for pos in range(max_len):
                tokens_at_pos = []
                for tokens in tokenized_texts:
                    if pos < len(tokens):
                        tokens_at_pos.append(tokens[pos])
                
                if tokens_at_pos:
                    # ê°€ì¥ ë§ì´ ì„ íƒëœ í† í°
                    token_counts = Counter(tokens_at_pos)
                    most_common_token = token_counts.most_common(1)[0][0]
                    final_tokens.append(most_common_token)
            
            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            try:
                final_text = reference_tokenizer.convert_tokens_to_string(final_tokens)
                ensemble_results.append(final_text.strip())
            except:
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ê°€ì¥ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                ensemble_results.append(texts_for_sample[0])
        
        log.info("í† í° ë‹¨ìœ„ í•˜ë“œ ë³´íŒ… ì™„ë£Œ")
        return ensemble_results
    
    def length_based_ensemble(self, input_texts, config):
        """
        ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”: ê° ëª¨ë¸ì˜ ê²°ê³¼ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ì„ íƒ
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        tokenizer = self.tokenizers[0]  # ê¸°ì¤€ í† í¬ë‚˜ì´ì €
        
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
        
        for text in tqdm(input_texts, desc="ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            try:
                # ì…ë ¥ í† í¬ë‚˜ì´ì œì´ì…˜
                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                # ê° ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ ê¸¸ì´ ê¸°ë°˜ ì„ íƒ
                model_results = []
                for model in self.models:
                    with torch.no_grad():
                        generated_ids = model.generate(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_length=config['inference']['generate_max_length'],
                            num_beams=config['inference']['num_beams'],
                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                            early_stopping=config['inference']['early_stopping']
                        )
                        
                        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=False)
                        
                        # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                        for token in config['inference']['remove_tokens']:
                            generated_text = generated_text.replace(token, " ")
                        
                        model_results.append(generated_text.strip())
                
                # ê¸¸ì´ ê¸°ë°˜ ì„ íƒ: ê°€ì¥ ê¸´ ê²°ê³¼ë¥¼ ì„ íƒ
                if model_results:
                    # ê°€ì¥ ê¸´ ê²°ê³¼ ì„ íƒ
                    longest_result = max(model_results, key=len)
                    results.append(longest_result)
                else:
                    results.append("")
                    
            except Exception as e:
                log.warning(f"ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì™„ë£Œ")
        return results
    
    def soft_voting_ensemble(self, input_texts, config):
        """
        ì§„ì§œ ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”: ëª¨ë¸ë“¤ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìƒì„±
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        tokenizer = self.tokenizers[0]  # ê¸°ì¤€ í† í¬ë‚˜ì´ì €
        
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        
        for text in tqdm(input_texts, desc="ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘"):
            try:
                # ì…ë ¥ í† í¬ë‚˜ì´ì œì´ì…˜
                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                # ê° ëª¨ë¸ì—ì„œ beam searchë¥¼ í†µí•´ ì—¬ëŸ¬ í›„ë³´ ìƒì„±
                model_candidates = []
                for model in self.models:
                    with torch.no_grad():
                        # beam searchë¡œ ì—¬ëŸ¬ í›„ë³´ ìƒì„±
                        outputs = model.generate(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_length=config['inference']['generate_max_length'],
                            num_beams=config['inference']['num_beams'],
                            num_return_sequences=min(3, config['inference']['num_beams']),  # ìµœëŒ€ 3ê°œ í›„ë³´
                            return_dict_in_generate=True,
                            output_scores=True,
                            early_stopping=config['inference']['early_stopping']
                        )
                        
                        # ê° í›„ë³´ì™€ ê·¸ ì ìˆ˜ë¥¼ ì €ì¥
                        candidates = []
                        for i, sequence in enumerate(outputs.sequences):
                            text_output = tokenizer.decode(sequence, skip_special_tokens=False)
                            
                            # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                            for token in config['inference']['remove_tokens']:
                                text_output = text_output.replace(token, " ")
                            
                            text_output = text_output.strip()
                            
                            # ì ìˆ˜ ê³„ì‚° (ê¸¸ì´ë¡œ ì •ê·œí™”ëœ í‰ê·  ì ìˆ˜)
                            if hasattr(outputs, 'sequences_scores') and len(outputs.sequences_scores) > i:
                                score = outputs.sequences_scores[i].item()
                            else:
                                # sequences_scoresê°€ ì—†ìœ¼ë©´ ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜ ì‚¬ìš©
                                score = len(text_output.split()) / config['inference']['generate_max_length']
                            
                            candidates.append((text_output, score))
                        
                        model_candidates.append(candidates)
                
                # ì†Œí”„íŠ¸ ë³´íŒ…: ê° ëª¨ë¸ì˜ ìµœê³  ì ìˆ˜ í›„ë³´ë“¤ ì¤‘ì—ì„œ í‰ê·  ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ê²ƒ ì„ íƒ
                all_candidates = []
                
                # ê° ëª¨ë¸ì˜ ëª¨ë“  í›„ë³´ë¥¼ ìˆ˜ì§‘
                for model_idx, candidates in enumerate(model_candidates):
                    for text_output, score in candidates:
                        all_candidates.append((text_output, score, model_idx))
                
                if all_candidates:
                    # ë™ì¼í•œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì ìˆ˜ í‰ê·  ê³„ì‚°
                    text_scores = {}
                    text_counts = {}
                    
                    for text_output, score, model_idx in all_candidates:
                        if text_output not in text_scores:
                            text_scores[text_output] = 0
                            text_counts[text_output] = 0
                        text_scores[text_output] += score
                        text_counts[text_output] += 1
                    
                    # í‰ê·  ì ìˆ˜ ê³„ì‚°
                    for text_output in text_scores:
                        text_scores[text_output] /= text_counts[text_output]
                    
                    # ê°€ì¥ ë†’ì€ í‰ê·  ì ìˆ˜ë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ ì„ íƒ
                    best_text = max(text_scores.keys(), key=lambda x: text_scores[x])
                    results.append(best_text)
                else:
                    results.append("")
                    
            except Exception as e:
                log.warning(f"ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì¤‘ ì˜¤ë¥˜ (fallback ì‚¬ìš©): {e}")
                results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì™„ë£Œ")
        return results
    
    def logit_level_ensemble(self, input_texts, config):
        """
        ìµœì í™”ëœ Logit ë ˆë²¨ ì•™ìƒë¸”: Nucleus Samplingê³¼ Beam Search ì ìš©
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            
        Returns:
            list: ì•™ìƒë¸” ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        return self.optimized_beam_search_ensemble(
            input_texts, 
            config,
            temperature=1.0,
            top_k=0,
            top_p=1.0,  # Nucleus Sampling ë¹„í™œì„±í™” - test_293 NaN ë¬¸ì œ í•´ê²°
            repetition_penalty=1.0
        )
    
    def optimized_beam_search_ensemble(self, input_texts, config, 
                                    temperature=1.0, 
                                    top_k=0, 
                                    top_p=0.9,
                                    repetition_penalty=1.0):
        """
        ìµœì í™”ëœ Beam Search ì•™ìƒë¸” (Nucleus Sampling ì ìš©)
        
        Args:
            input_texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            temperature: ì˜¨ë„ íŒŒë¼ë¯¸í„°
            top_k: Top-K í•„í„°ë§
            top_p: Nucleus sampling
            repetition_penalty: ë°˜ë³µ í˜ë„í‹°
            
        Returns:
            list: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        tokenizer = self.tokenizers[0]
        max_length = config['inference']['generate_max_length']
        num_beams = config['inference']['num_beams']
        
        log.info(f"ìµœì í™”ëœ Logit ì•™ìƒë¸” ì‹œì‘: top_p={top_p}, num_beams={num_beams}")
        
        for idx, text in enumerate(tqdm(input_texts, desc="ìµœì í™”ëœ Logit ì•™ìƒë¸” ì²˜ë¦¬ ì¤‘")):
            try:
                # ë””ë²„ê¹…: í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ìƒ˜í”Œ ë¡œê·¸  
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ì²˜ë¦¬ ì‹œì‘")
                log.info(f"ğŸ” [DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
                log.info(f"ğŸ” [DEBUG] ì…ë ¥ í…ìŠ¤íŠ¸ ì• 100ì: {text[:100]}")
                
                # ì…ë ¥ í† í°í™”
                inputs = tokenizer(
                    text,
                    return_tensors="pt",
                    max_length=config['tokenizer']['encoder_max_len'],
                    truncation=True,
                    padding=True
                ).to(self.device)
                
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} í† í°í™” ì„±ê³µ: input_ids shape={inputs['input_ids'].shape}")
                
                # ê° ëª¨ë¸ì˜ encoder ì¶œë ¥ ë¯¸ë¦¬ ê³„ì‚° (ê¹Šì€ ë³µì‚¬ë¡œ ë…ë¦½ì„± ë³´ì¥)
                encoder_outputs_list = []
                for model_idx, model in enumerate(self.models):
                    with torch.no_grad():
                        encoder_outputs = model.get_encoder()(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask']
                        )
                        # ê¹Šì€ ë³µì‚¬ë¡œ ë…ë¦½ì„± ë³´ì¥
                        encoder_outputs_copy = encoder_outputs.last_hidden_state.clone().detach()
                        encoder_outputs_list.append(encoder_outputs_copy)
                        
                        log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ëª¨ë¸ {model_idx+1} encoder ì¶œë ¥ shape: {encoder_outputs_copy.shape}")
                        
                        # ë©”ëª¨ë¦¬ ì •ë¦¬
                        del encoder_outputs
                
                # Beam Search ì´ˆê¸°í™”
                decoder_start_token_id = tokenizer.bos_token_id
                if decoder_start_token_id is None:
                    decoder_start_token_id = tokenizer.eos_token_id
                
                batch_size = 1
                beam_size = num_beams
                
                sequences = torch.full((batch_size * beam_size, 1), decoder_start_token_id, device=self.device)
                beam_scores = torch.zeros(batch_size * beam_size, device=self.device)
                beam_scores[1:] = -float('inf')
                
                eos_token_id = tokenizer.eos_token_id
                finished_sequences = []
                
                # Beam Search ë£¨í”„ (Nucleus Sampling ì ìš©)
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} Beam Search ì‹œì‘ - max_length: {max_length}, num_beams: {num_beams}")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} decoder_start_token_id: {decoder_start_token_id}, eos_token_id: {eos_token_id}")
                
                for step in range(max_length - 1):
                    if step % 10 == 0 or step < 5:  # ì²˜ìŒ 5ë‹¨ê³„ì™€ 10ë‹¨ê³„ë§ˆë‹¤ ë¡œê¹…
                        log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} step {step}: finished_sequences={len(finished_sequences)}, beam_size={beam_size}")
                    
                    if len(finished_sequences) >= beam_size:
                        log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} step {step}: ì¶©ë¶„í•œ finished_sequencesë¡œ ì¡°ê¸° ì¢…ë£Œ")
                        break
                    
                    current_sequences = sequences[beam_scores > -float('inf')]
                    current_scores = beam_scores[beam_scores > -float('inf')]
                    
                    if len(current_sequences) == 0:
                        log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} step {step}: current_sequencesê°€ ë¹„ì–´ì„œ ì¢…ë£Œ")
                        break
                    
                    # ê° ëª¨ë¸ì—ì„œ logits ê³„ì‚°
                    all_next_logits = []
                    
                    for model_idx, model in enumerate(self.models):
                        with torch.no_grad():
                            decoder_outputs = model.get_decoder()(
                                input_ids=current_sequences,
                                encoder_hidden_states=encoder_outputs_list[model_idx].expand(len(current_sequences), -1, -1),
                                encoder_attention_mask=inputs['attention_mask'].expand(len(current_sequences), -1)
                            )
                            
                            logits = model.lm_head(decoder_outputs.last_hidden_state)
                            next_token_logits = logits[:, -1, :]
                            all_next_logits.append(next_token_logits)
                    
                    # ëª¨ë“  ëª¨ë¸ì˜ logits í‰ê· 
                    ensemble_logits = torch.stack(all_next_logits).mean(dim=0)
                    
                    # === Nucleus Sampling ì ìš© ===
                    if top_p < 1.0:
                        for beam_idx in range(ensemble_logits.size(0)):
                            sorted_logits, sorted_indices = torch.sort(ensemble_logits[beam_idx], descending=True)
                            sorted_probs = torch.softmax(sorted_logits, dim=-1)
                            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                            
                            # nucleus ë°–ì˜ í† í°ë“¤ ì œê±°
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
                            sorted_indices_to_remove[0] = 0
                            
                            indices_to_remove = sorted_indices[sorted_indices_to_remove]
                            
                            # ë””ë²„ê¹…: nucleus sampling ìƒíƒœ ë¡œê¹…
                            if step < 5:  # ì²˜ìŒ 5 ìŠ¤í…ë§Œ ë¡œê¹…
                                log.info(f"ğŸ” [NUCLEUS] ìƒ˜í”Œ {idx} step {step} beam {beam_idx}")
                                log.info(f"ğŸ” [NUCLEUS] ì›ë³¸ logits ë²”ìœ„: [{ensemble_logits[beam_idx].min():.3f}, {ensemble_logits[beam_idx].max():.3f}]")
                                log.info(f"ğŸ” [NUCLEUS] top 5 logits: {sorted_logits[:5].tolist()}")
                                log.info(f"ğŸ” [NUCLEUS] top 5 probs: {sorted_probs[:5].tolist()}")
                                log.info(f"ğŸ” [NUCLEUS] cumulative_probs[:10]: {cumulative_probs[:10].tolist()}")
                                log.info(f"ğŸ” [NUCLEUS] ì œê±°í•  í† í° ìˆ˜: {len(indices_to_remove)}")
                                log.info(f"ğŸ” [NUCLEUS] ë‚¨ì€ í† í° ìˆ˜: {(ensemble_logits[beam_idx] > -float('inf')).sum().item()}")
                            
                            ensemble_logits[beam_idx, indices_to_remove] = -float('inf')
                            
                            # ì¶”ê°€ ì²´í¬: ëª¨ë“  í† í°ì´ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            valid_tokens = (ensemble_logits[beam_idx] > -float('inf')).sum().item()
                            if valid_tokens == 0:
                                log.error(f"ğŸš¨ [NUCLEUS] ìƒ˜í”Œ {idx} step {step} beam {beam_idx}: ëª¨ë“  í† í°ì´ ì œê±°ë¨!")
                                # ì‘ê¸‰ì²˜ì¹˜: ìµœê³  í™•ë¥  í† í° í•˜ë‚˜ëŠ” ìœ ì§€
                                best_token_idx = torch.argmax(sorted_logits)
                                ensemble_logits[beam_idx, sorted_indices[best_token_idx]] = sorted_logits[best_token_idx]
                    
                    # Log probabilities ê³„ì‚°
                    next_token_log_probs = torch.log_softmax(ensemble_logits, dim=-1)
                    
                    # ìƒˆë¡œìš´ beam í›„ë³´ ìƒì„±
                    vocab_size = next_token_log_probs.size(-1)
                    next_scores = current_scores.unsqueeze(1) + next_token_log_probs
                    next_scores = next_scores.view(-1)
                    
                    # Top-k ì„ íƒ
                    top_scores, top_indices = torch.topk(next_scores, min(beam_size * 2, len(next_scores)))
                    
                    # ìƒˆ beam êµ¬ì„±
                    new_sequences = []
                    new_scores = []
                    
                    for score, token_idx in zip(top_scores, top_indices):
                        beam_idx = token_idx // vocab_size
                        token_id = token_idx % vocab_size
                        
                        new_seq = torch.cat([
                            current_sequences[beam_idx],
                            torch.tensor([token_id], device=self.device)
                        ])
                        
                        # EOS í† í° ì²´í¬
                        if token_id == eos_token_id:
                            finished_sequences.append((new_seq, score.item()))
                        else:
                            new_sequences.append(new_seq)
                            new_scores.append(score)
                            
                        if len(new_sequences) >= beam_size:
                            break
                    
                    if not new_sequences:
                        break
                    
                    # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì—…ë°ì´íŠ¸
                    max_len = max(len(seq) for seq in new_sequences)
                    sequences = torch.full((beam_size, max_len), tokenizer.pad_token_id, device=self.device)
                    beam_scores = torch.full((beam_size,), -float('inf'), device=self.device)
                    
                    for i, (seq, score) in enumerate(zip(new_sequences[:beam_size], new_scores[:beam_size])):
                        sequences[i, :len(seq)] = seq
                        beam_scores[i] = score
                
                # Beam Search ì™„ë£Œ í›„ ë¡œê¹…
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} Beam Search ì™„ë£Œ - finished_sequences: {len(finished_sequences)}")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} beam_scores ìœ íš¨ ê°œìˆ˜: {torch.sum(beam_scores > -float('inf')).item()}")
                
                # ìµœê³  ì ìˆ˜ ì‹œí€€ìŠ¤ ì„ íƒ
                if finished_sequences:
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} finished_sequences ì„ íƒ ì‹œì‘: {len(finished_sequences)}ê°œ í›„ë³´")
                    for i, (seq, score) in enumerate(finished_sequences[:3]):  # ìƒìœ„ 3ê°œë§Œ ë¡œê·¸
                        seq_preview = seq.tolist()[:15] if len(seq) > 15 else seq.tolist()
                        log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} finished_seq[{i}]: {seq_preview} (score: {score})")
                    
                    best_sequence, best_score = max(finished_sequences, key=lambda x: x[1])
                    
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ì„ íƒëœ best_score: {best_score}")
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ì„ íƒëœ best_sequence: {best_sequence.tolist()}")
                else:
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} finished_sequencesê°€ ë¹„ì–´ì„œ beam_scoresì—ì„œ ì„ íƒ")
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} beam_scores: {beam_scores.tolist()}")
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} sequences shape: {sequences.shape}")
                    
                    best_idx = torch.argmax(beam_scores)
                    best_sequence = sequences[best_idx]
                    
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ì„ íƒëœ best_idx: {best_idx}")
                    log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ì„ íƒëœ best_sequence: {best_sequence.tolist()}")
                
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} best_sequence shape: {best_sequence.shape}")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} best_sequence ì‹¤ì œ ë‚´ìš©: {best_sequence}")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} best_sequenceê°€ ë¹„ì–´ìˆëŠ”ê°€? {len(best_sequence) == 0}")
                
                # CRITICAL: best_sequenceê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° fallback ì²˜ë¦¬
                if len(best_sequence) == 0 or best_sequence.dim() == 0:
                    log.error(f"ğŸš¨ [CRITICAL] ìƒ˜í”Œ {idx} best_sequenceê°€ ë¹„ì–´ìˆìŒ! fallback ì²˜ë¦¬")
                    results.append("")
                    continue
                
                # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (baseline.pyì™€ ë™ì¼í•˜ê²Œ íŠ¹ìˆ˜ í† í° ìœ ì§€)
                generated_text = tokenizer.decode(best_sequence, skip_special_tokens=False)
                
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ë””ì½”ë”© ê²°ê³¼: '{generated_text}'")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} best_sequence ë‚´ìš©: {best_sequence.tolist()}")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} best_sequence ê¸¸ì´: {len(best_sequence)}")
                
                # ë¶ˆí•„ìš”í•œ í† í° ì œê±°
                original_text = generated_text
                for token in config['inference']['remove_tokens']:
                    generated_text = generated_text.replace(token, " ")
                
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} í† í° ì œê±° ì „: '{original_text}'")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} í† í° ì œê±° í›„: '{generated_text}'")
                log.info(f"ğŸ” [DEBUG] ìƒ˜í”Œ {idx} ìµœì¢… ê²°ê³¼: '{generated_text.strip()}'")
                
                results.append(generated_text.strip())
                
            except Exception as e:
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸
                log.error(f"ğŸš¨ [ERROR] ìƒ˜í”Œ {idx} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ!")
                log.error(f"ğŸš¨ [ERROR] ì˜ˆì™¸ ìœ í˜•: {type(e).__name__}")
                log.error(f"ğŸš¨ [ERROR] ì˜ˆì™¸ ë©”ì‹œì§€: {str(e)}")
                import traceback
                log.error(f"ğŸš¨ [ERROR] ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                
                log.warning(f"ìµœì í™”ëœ Logit ì•™ìƒë¸” ì˜¤ë¥˜ (ìƒ˜í”Œ {idx}): {e}")
                # Fallback: ì²« ë²ˆì§¸ ëª¨ë¸ì˜ beam search ê²°ê³¼ ì‚¬ìš©
                try:
                    log.info(f"ğŸ”„ [FALLBACK] ìƒ˜í”Œ {idx} fallback ì‹œë„: ì²« ë²ˆì§¸ ëª¨ë¸ ë‹¨ë… ìƒì„±")
                    with torch.no_grad():
                        output_ids = self.models[0].generate(
                            input_ids=inputs['input_ids'],
                            attention_mask=inputs['attention_mask'],
                            max_length=config['inference']['generate_max_length'],
                            num_beams=config['inference']['num_beams'],
                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                            early_stopping=config['inference']['early_stopping']
                        )
                    fallback_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)
                    for token in config['inference']['remove_tokens']:
                        fallback_text = fallback_text.replace(token, " ")
                    
                    log.info(f"ğŸ”„ [FALLBACK] ìƒ˜í”Œ {idx} fallback ì„±ê³µ: '{fallback_text.strip()}'")
                    results.append(fallback_text.strip())
                except Exception as fallback_e:
                    log.error(f"ğŸš¨ [FALLBACK ERROR] ìƒ˜í”Œ {idx} fallbackë„ ì‹¤íŒ¨: {fallback_e}")
                    log.error(f"ğŸš¨ [FALLBACK ERROR] ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´")
                    results.append("")  # ë¹ˆ ë¬¸ìì—´ë¡œ fallback
        
        log.info("ìµœì í™”ëœ Logit ì•™ìƒë¸” ì™„ë£Œ")
        return results
    
    def evaluate_on_validation(self, val_data_path):
        """
        ê²€ì¦ ë°ì´í„°ë¡œ ì•™ìƒë¸” ë° ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        
        Args:
            val_data_path: ê²€ì¦ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            dict: í‰ê°€ ê²°ê³¼ (ê°œë³„ ëª¨ë¸ ì ìˆ˜, ì•™ìƒë¸” ì ìˆ˜)
        """
        log.info(f"ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘: {val_data_path}")
        
        # ê²€ì¦ ë°ì´í„° ë¡œë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©)
        val_df = pd.read_csv(val_data_path)
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²˜ìŒ 50ê°œë§Œ ì‚¬ìš©
        val_df = val_df.head(50)
        input_texts = val_df['dialogue'].tolist()
        reference_summaries = val_df['summary'].tolist()
        log.info(f"ê²€ì¦ ë°ì´í„° ë¡œë“œ ì™„ë£¼: {len(input_texts)}ê°œ ìƒ˜í”Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)")
        
        # ê° ëª¨ë¸ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ìƒì„±
        all_generated_texts = []
        individual_scores = []
        
        for i, (model, tokenizer, config) in enumerate(zip(self.models, self.tokenizers, self.configs)):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.models)} ê²€ì¦ ì ìˆ˜ ê³„ì‚° ì‹œì‘ (baseline.py ë°©ì‹)...")
            
            # baseline.py ë°©ì‹ìœ¼ë¡œ ì •í™•í•œ ê²€ì¦ ì ìˆ˜ ê³„ì‚°
            rouge_scores = evaluate_single_model_with_baseline(model, tokenizer, config)
            individual_scores.append({
                'model_index': i + 1,
                'model_metadata': self.metadata_list[i],
                'rouge_scores': rouge_scores
            })
            
            log.info(f"ëª¨ë¸ {i+1} ê²€ì¦ ì ìˆ˜ (baseline.py ë°©ì‹) - ROUGE-1: {rouge_scores['rouge-1']:.6f}, "
                    f"ROUGE-2: {rouge_scores['rouge-2']:.6f}, ROUGE-L: {rouge_scores['rouge-l']:.6f}")
            
            # ì•™ìƒë¸”ìš© í† í° ID ì¶”ë¡  ë°ì´í„° ì¤€ë¹„ (í† í° ë ˆë²¨ ì•™ìƒë¸”ì„ ìœ„í•¨)
            generated_token_ids = self.generate_token_ids_with_single_model(model, tokenizer, config, input_texts)
            all_generated_texts.append(generated_token_ids)
        
        # ì„¸ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ëª¨ë‘ í…ŒìŠ¤íŠ¸
        log.info("\n=== í† í° ID ë ˆë²¨ í•˜ë“œ ë³´íŒ… vs ì†Œí”„íŠ¸ ë³´íŒ… vs ê¸¸ì´ ê¸°ë°˜ ë¹„êµ ===")
        
        # 1. í† í° ID ë ˆë²¨ í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”
        log.info("í† í° ID ë ˆë²¨ í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        hard_voting_results = self.token_id_level_hard_voting(all_generated_texts, self.tokenizers[0])
        
        # 2. ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        soft_voting_results = self.soft_voting_ensemble(input_texts, self.configs[0])
        
        # 3. ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
        length_based_results = self.length_based_ensemble(input_texts, self.configs[0])
        
        # 4. Logit ë ˆë²¨ ì•™ìƒë¸”
        log.info("Logit ë ˆë²¨ ì•™ìƒë¸” ì‹œì‘...")
        logit_level_results = self.logit_level_ensemble(input_texts, self.configs[0])
        
        # ROUGE ê³„ì‚° í•¨ìˆ˜ ì •ì˜ (baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
        def calculate_rouge_scores(predictions, references, method_name):
            from rouge import Rouge
            rouge = Rouge()
            
            # baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í† í° ì œê±° (ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´)
            replaced_predictions = predictions.copy()
            replaced_references = references.copy()
            remove_tokens = self.configs[0]['inference']['remove_tokens']
            for token in remove_tokens:
                replaced_predictions = [sentence.replace(token, " ") for sentence in replaced_predictions]
                replaced_references = [sentence.replace(token, " ") for sentence in replaced_references]
            
            # baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”
            cleaned_predictions = []
            cleaned_references = []
            for pred, ref in zip(replaced_predictions, replaced_references):
                # ê³µë°± ì •ë¦¬ (baseline.pyì˜ clean_up_tokenization_spaces=True íš¨ê³¼ ëª¨ë°©)
                pred_clean = " ".join(pred.split()).strip()
                ref_clean = " ".join(ref.split()).strip()
                    
                cleaned_predictions.append(pred_clean)
                cleaned_references.append(ref_clean)
            
            try:
                # ë¹ˆ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ rouge ê³„ì‚° ì‹¤íŒ¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
                final_predictions = []
                final_references = []
                for pred, ref in zip(cleaned_predictions, cleaned_references):
                    if pred.strip() and ref.strip():
                        final_predictions.append(pred)
                        final_references.append(ref)
                    else:
                        # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° "empty"ë¡œ ëŒ€ì²´
                        final_predictions.append("empty" if not pred.strip() else pred)
                        final_references.append("empty" if not ref.strip() else ref)
                
                rouge_results = rouge.get_scores(final_predictions, final_references, avg=True)
                rouge_scores = {key: value["f"] for key, value in rouge_results.items()}
                # rouge-avg ê³„ì‚° ì¶”ê°€
                rouge_avg = (rouge_scores['rouge-1'] + rouge_scores['rouge-2'] + rouge_scores['rouge-l']) / 3
                rouge_scores['rouge-avg'] = rouge_avg
                
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-1: {rouge_scores['rouge-1']:.4f}, "
                        f"ROUGE-2: {rouge_scores['rouge-2']:.4f}, ROUGE-L: {rouge_scores['rouge-l']:.4f}, "
                        f"ROUGE-avg: {rouge_scores['rouge-avg']:.4f}")
                return rouge_scores
            except Exception as e:
                log.warning(f"{method_name} ROUGE ê³„ì‚° ì˜¤ë¥˜: {e}")
                return {'rouge-1': 0.0, 'rouge-2': 0.0, 'rouge-l': 0.0, 'rouge-avg': 0.0}
        
        # 3. ë„¤ ë°©ì‹ì˜ ROUGE ì ìˆ˜ ê³„ì‚°
        hard_voting_scores = calculate_rouge_scores(hard_voting_results, reference_summaries, "í•˜ë“œ ë³´íŒ…")
        soft_voting_scores = calculate_rouge_scores(soft_voting_results, reference_summaries, "ì†Œí”„íŠ¸ ë³´íŒ…")
        length_based_scores = calculate_rouge_scores(length_based_results, reference_summaries, "ê¸¸ì´ ê¸°ë°˜")
        logit_level_scores = calculate_rouge_scores(logit_level_results, reference_summaries, "Logit ë ˆë²¨")
        
        # 4. ë¹„êµ ê²°ê³¼ ì¶œë ¥
        log.info("\n=== ì•™ìƒë¸” ë°©ì‹ ë¹„êµ ê²°ê³¼ ===")
        log.info(f"í•˜ë“œ ë³´íŒ… ROUGE-avg: {hard_voting_scores['rouge-avg']:.4f}")
        log.info(f"ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-avg: {soft_voting_scores['rouge-avg']:.4f}")
        log.info(f"ê¸¸ì´ ê¸°ë°˜ ROUGE-avg: {length_based_scores['rouge-avg']:.4f}")
        log.info(f"Logit ë ˆë²¨ ROUGE-avg: {logit_level_scores['rouge-avg']:.4f}")
        
        # ê°€ì¥ ë‚˜ì€ ë°©ì‹ ì„ íƒ
        all_scores = {
            "í•˜ë“œ ë³´íŒ…": (hard_voting_scores, hard_voting_results),
            "ì†Œí”„íŠ¸ ë³´íŒ…": (soft_voting_scores, soft_voting_results),
            "ê¸¸ì´ ê¸°ë°˜": (length_based_scores, length_based_results),
            "Logit ë ˆë²¨": (logit_level_scores, logit_level_results)
        }
        
        best_method = max(all_scores.keys(), key=lambda x: all_scores[x][0]['rouge-avg'])
        ensemble_rouge_scores = all_scores[best_method][0]
        
        log.info(f"{best_method}ì´ ê°€ì¥ ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤!")
        
        evaluation_results = {
            'individual_model_scores': individual_scores,
            'hard_voting_scores': hard_voting_scores,
            'soft_voting_scores': soft_voting_scores,
            'length_based_scores': length_based_scores,
            'logit_level_scores': logit_level_scores,
            'ensemble_scores': ensemble_rouge_scores,
            'best_ensemble_method': best_method,
            'num_validation_samples': len(input_texts)
        }
        
        log.info("ê²€ì¦ ë°ì´í„° í‰ê°€ ì™„ë£Œ (baseline.py ë°©ì‹ ì‚¬ìš©)")
        return evaluation_results
    
    def run_ensemble(self, test_data_path):
        """
        í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹¤í–‰
        
        Args:
            test_data_path: í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ
            
        Returns:
            tuple: (ensemble_result_df, individual_results_list)
        """
        log.info(f"ì•™ìƒë¸” ì¶”ë¡  ì‹œì‘: {test_data_path}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì¼ë¶€ë§Œ ì‚¬ìš©)
        test_df = pd.read_csv(test_data_path)
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²˜ìŒ 20ê°œë§Œ ì‚¬ìš©
        test_df = test_df.head(20)
        input_texts = test_df['dialogue'].tolist()
        log.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£¼: {len(input_texts)}ê°œ ìƒ˜í”Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)")
        
        # ê°œë³„ ëª¨ë¸ë“¤ë¡œ ì¶”ë¡  ìˆ˜í–‰
        all_generated_texts = []
        
        for i, (model, tokenizer, config) in enumerate(zip(self.models, self.tokenizers, self.configs)):
            log.info(f"ëª¨ë¸ {i+1}/{len(self.models)} ì¶”ë¡  ì‹œì‘...")
            log.info(f"ëª¨ë¸ ì„¤ì • - max_length: {config['inference']['generate_max_length']}, "
                    f"num_beams: {config['inference']['num_beams']}")
            
            generated_texts = self.generate_with_single_model(model, tokenizer, config, input_texts)
            all_generated_texts.append(generated_texts)
            
            log.info(f"ëª¨ë¸ {i+1} ì¶”ë¡  ì™„ë£Œ")
        
        # ì„¸ ê°€ì§€ ì•™ìƒë¸” ë°©ì‹ ëª¨ë‘ ìˆ˜í–‰
        log.info("\n=== í•˜ë“œ ë³´íŒ… vs ì†Œí”„íŠ¸ ë³´íŒ… vs ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ìˆ˜í–‰ ===")
        
        # 1. í•˜ë“œ ë³´íŒ… ì•™ìƒë¸”
        log.info("í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        hard_voting_results = self.token_level_hard_voting(all_generated_texts, self.tokenizers[0])
        
        # 2. ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸”
        log.info("ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ì‹œì‘...")
        soft_voting_results = self.soft_voting_ensemble(input_texts, self.configs[0])
        
        # 3. ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸”
        log.info("ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ì‹œì‘...")
        length_based_results = self.length_based_ensemble(input_texts, self.configs[0])
        
        # 4. Logit ë ˆë²¨ ì•™ìƒë¸”
        log.info("Logit ë ˆë²¨ ì•™ìƒë¸” ì‹œì‘...")
        logit_level_results = self.logit_level_ensemble(input_texts, self.configs[0])
        
        # 5. ë„¤ ë°©ì‹ì˜ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        hard_voting_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': hard_voting_results
        })
        
        soft_voting_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': soft_voting_results
        })
        
        length_based_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': length_based_results
        })
        
        logit_level_df = pd.DataFrame({
            'fname': test_df['fname'],
            'summary': logit_level_results
        })
        
        log.info("ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ (í•˜ë“œ ë³´íŒ… & ì†Œí”„íŠ¸ ë³´íŒ… & ê¸¸ì´ ê¸°ë°˜ & Logit ë ˆë²¨)")
        
        # ë„¤ ë°©ì‹ì˜ ê²°ê³¼ë¥¼ ëª¨ë‘ ë°˜í™˜
        ensemble_results = {
            'hard_voting': hard_voting_df,
            'soft_voting': soft_voting_df,
            'length_based': length_based_df,
            'logit_level': logit_level_df,
            'individual_results': all_generated_texts
        }
        
        return ensemble_results, all_generated_texts
    
    def evaluate_individual_models(self, val_data_path):
        """
        baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê°œë³„ ëª¨ë¸ë“¤ì„ í‰ê°€í•©ë‹ˆë‹¤.
        """
        log.info("ê°œë³„ ëª¨ë¸ í‰ê°€ ì‹œì‘ (baseline.py ë°©ì‹)")
        
        # ì´ë¯¸ í˜„ì¬ íŒŒì¼ì— ìˆëŠ” í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©
        import pandas as pd
        
        try:
            individual_scores = []
            
            for i, (model, tokenizer, config) in enumerate(zip(self.models, self.tokenizers, self.configs)):
                log.info(f"ëª¨ë¸ {i+1}/{len(self.models)} í‰ê°€ ì¤‘...")
                
                # baseline.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í‰ê°€
                eval_results = evaluate_single_model_with_baseline(model, tokenizer, config)
                
                individual_scores.append({
                    'model_index': i + 1,
                    'model_metadata': getattr(model, 'metadata', {}),
                    'rouge_scores': eval_results
                })
                
                log.info(f"ëª¨ë¸ {i+1} í‰ê°€ ì™„ë£Œ: ROUGE-avg {eval_results['rouge-avg']:.4f}")
            
            return {'individual_model_scores': individual_scores}
            
        except Exception as e:
            log.error(f"ê°œë³„ ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            return {'individual_model_scores': []}

def run_single_method(method_name):
    """
    ê°œë³„ ì•™ìƒë¸” ë°©ì‹ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        method_name: ì‹¤í–‰í•  ë°©ì‹ ('hard_voting', 'soft_voting', 'length_based', 'realtime_token')
    """
    log.info(f"ğŸ¯ ê°œë³„ ë°©ì‹ ì‹¤í–‰: {method_name}")
    
    # ê³µí†µ í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    existing_model_paths = get_model_paths()
    if not existing_model_paths:
        return
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ {method_name} ì§„í–‰")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸” ë°©ì‹
    if method_name == "realtime_token":
        ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
        ensemble.load_models()
        
        # ê²€ì¦ ë°ì´í„° í‰ê°€
        val_data_path = "../../input/data/dev.csv"
        if os.path.exists(val_data_path):
            log.info("ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘")
            evaluation_results = ensemble.evaluate_on_validation(val_data_path)
            if evaluation_results:
                scores = evaluation_results['realtime_token_ensemble_scores']
                log.info(f"{method_name} ê²€ì¦ ì ìˆ˜ - ROUGE-avg: {scores['rouge-avg']:.4f}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ 
        test_data_path = "../../input/data/test.csv"
        if os.path.exists(test_data_path):
            ensemble_df, generation_time = ensemble.run_ensemble(test_data_path)
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = "./ensemble_results"
            os.makedirs(results_dir, exist_ok=True)
            
            result_path = os.path.join(results_dir, f"ensemble_{method_name}_{timestamp}.csv")
            ensemble_df.to_csv(result_path, index=False, encoding='utf-8')
            log.info(f"{method_name} ê²°ê³¼ ì €ì¥: {result_path}")
            log.info(f"{method_name} ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
    
    # í›„ì²˜ë¦¬ ë°©ì‹ë“¤ (hard_voting, soft_voting, length_based)
    else:
        ensemble = PostProcessingEnsemble(existing_model_paths, device=device)
        ensemble.load_models()
        
        # ê²€ì¦ ë°ì´í„°ë¡œ ê°œë³„ ë°©ì‹ í‰ê°€
        val_data_path = "../../input/data/dev.csv"
        if os.path.exists(val_data_path):
            log.info("ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹œì‘")
            val_df = pd.read_csv(val_data_path)
            val_df_sample = val_df  # baseline.pyì™€ ë™ì¼í•˜ê²Œ ì „ì²´ ë°ì´í„° ì‚¬ìš©
            input_texts = val_df_sample['dialogue'].tolist()
            reference_summaries = val_df_sample['summary'].tolist()
            
            # ì„ íƒí•œ ë°©ì‹ìœ¼ë¡œë§Œ ìƒì„±
            if method_name == "hard_voting":
                # ëª¨ë“  ëª¨ë¸ë¡œ ìƒì„± í›„ í•˜ë“œ ë³´íŒ…
                generated_texts_list = []
                for model, tokenizer, config in zip(ensemble.models, ensemble.tokenizers, ensemble.configs):
                    texts = ensemble.generate_with_single_model(model, tokenizer, config, input_texts)
                    generated_texts_list.append(texts)
                results = ensemble.token_level_hard_voting(generated_texts_list, ensemble.tokenizers[0])
                
            elif method_name == "soft_voting":
                results = ensemble.soft_voting_ensemble(input_texts, ensemble.configs[0])
                
            elif method_name == "length_based":
                results = ensemble.length_based_ensemble(input_texts, ensemble.configs[0])
                
            elif method_name == "logit_level":
                results = ensemble.logit_level_ensemble(input_texts, ensemble.configs[0])
            
            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ë„ í•¨ê»˜ ê³„ì‚° (baseline.py ë°©ì‹)
            log.info("ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ê³„ì‚° ì¤‘ (baseline.py ë°©ì‹)...")
            individual_scores = ensemble.evaluate_individual_models(val_data_path)['individual_model_scores']
            
            # ì•™ìƒë¸” ì ìˆ˜ë„ baseline.py ë°©ì‹ìœ¼ë¡œ ê³„ì‚°
            log.info("ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚° ì¤‘ (baseline.py ë°©ì‹)...")
            rouge_scores = evaluate_ensemble_results_with_baseline(
                results, reference_summaries, ensemble.configs[0], ensemble.tokenizers[0]
            )
            
            # ê°œë³„ ëª¨ë¸ ì ìˆ˜ëŠ” ì´ë¯¸ baseline.py ë°©ì‹ìœ¼ë¡œ ê³„ì‚°ë¨
            
            # ê²°ê³¼ ì¶œë ¥
            log.info("="*80)
            log.info(f"ğŸ¯ {method_name.upper()} ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ({len(val_df_sample)}ê°œ ìƒ˜í”Œ, baseline.py ë°©ì‹)")
            log.info("="*80)
            
            # ê°œë³„ ëª¨ë¸ ì ìˆ˜ ì¶œë ¥ (baseline.py ë°©ì‹ìœ¼ë¡œ ê³„ì‚°ëœ ì ìˆ˜)
            log.info("ğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ (baseline.py ë°©ì‹):")
            best_individual_score = 0
            best_model_idx = 0
            for i, score_info in enumerate(individual_scores):
                scores = score_info['rouge_scores']
                log.info(f"  ëª¨ë¸ {i+1}: ROUGE-avg {scores['rouge-avg']:.4f}")
                if scores['rouge-avg'] > best_individual_score:
                    best_individual_score = scores['rouge-avg']
                    best_model_idx = i
            
            # ì•™ìƒë¸” ì ìˆ˜ ì¶œë ¥
            log.info(f"ğŸš€ {method_name.upper()} ì•™ìƒë¸”: ROUGE-avg {rouge_scores['rouge-avg']:.4f}")
            
            # ì„±ëŠ¥ ë¹„êµ
            improvement = rouge_scores['rouge-avg'] - best_individual_score
            improvement_pct = (improvement / best_individual_score) * 100 if best_individual_score > 0 else 0
            
            log.info("="*80)
            log.info("ğŸ“ˆ ì„±ëŠ¥ ë¶„ì„:")
            log.info(f"  ìµœê³  ê°œë³„ ëª¨ë¸ (ëª¨ë¸ {best_model_idx+1}): {best_individual_score:.4f}")
            log.info(f"  {method_name.upper()} ì•™ìƒë¸”:             {rouge_scores['rouge-avg']:.4f}")
            log.info(f"  ì„±ëŠ¥ ì°¨ì´:                      {improvement:+.4f} ({improvement_pct:+.1f}%)")
            
            if improvement > 0:
                log.info("  âœ… ì•™ìƒë¸”ì´ ê°œë³„ ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤!")
            elif abs(improvement) < 0.01:
                log.info("  ğŸ¤ ì•™ìƒë¸”ê³¼ ê°œë³„ ëª¨ë¸ì´ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            else:
                log.info("  âš ï¸  ê°œë³„ ëª¨ë¸ì´ ì•™ìƒë¸”ë³´ë‹¤ ë” ì¢‹ìŠµë‹ˆë‹¤.")
            log.info("="*80)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ 
        test_data_path = "../../input/data/test.csv"
        if os.path.exists(test_data_path):
            test_df = pd.read_csv(test_data_path)
            test_df_sample = test_df  # baseline.pyì™€ ë™ì¼í•˜ê²Œ ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
            test_input_texts = test_df_sample['dialogue'].tolist()
            
            # ì„ íƒí•œ ë°©ì‹ìœ¼ë¡œë§Œ ìƒì„±
            if method_name == "hard_voting":
                generated_texts_list = []
                for model, tokenizer, config in zip(ensemble.models, ensemble.tokenizers, ensemble.configs):
                    texts = ensemble.generate_with_single_model(model, tokenizer, config, test_input_texts)
                    generated_texts_list.append(texts)
                final_results = ensemble.token_level_hard_voting(generated_texts_list, ensemble.tokenizers[0])
                
            elif method_name == "soft_voting":
                final_results = ensemble.soft_voting_ensemble(test_input_texts, ensemble.configs[0])
                
            elif method_name == "length_based":
                final_results = ensemble.length_based_ensemble(test_input_texts, ensemble.configs[0])
                
            elif method_name == "logit_level":
                final_results = ensemble.logit_level_ensemble(test_input_texts, ensemble.configs[0])
            
            # ê²°ê³¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_dir = "./ensemble_results"
            os.makedirs(results_dir, exist_ok=True)
            
            result_df = pd.DataFrame({
                'fname': test_df_sample['fname'],
                'summary': final_results
            })
            
            result_path = os.path.join(results_dir, f"ensemble_{method_name}_{timestamp}.csv")
            result_df.to_csv(result_path, index=False, encoding='utf-8')
            log.info(f"{method_name} ê²°ê³¼ ì €ì¥: {result_path}")
    
    log.info(f"ğŸ‰ {method_name} ì‹¤í–‰ ì™„ë£Œ!")

def main(ensemble_strategy="comprehensive"):
    """
    ì•™ìƒë¸” ì¶”ë¡  ë©”ì¸ í•¨ìˆ˜
    
    Args:
        ensemble_strategy: ì•™ìƒë¸” ì „ëµ ('comprehensive', 'hard_voting', 'soft_voting', 'length_based', 'realtime_token', 'post_token_voting', 'realtime_token_ensemble')
    """
    
    # ğŸ”¬ ì¢…í•© ì‹¤í—˜ ì‹¤í–‰ (ëª¨ë“  ë°©ì‹ ë¹„êµ)
    if ensemble_strategy == "comprehensive":
        return main_comprehensive_experiment()
    
    # ğŸ¯ ê°œë³„ ë°©ì‹ ì‹¤í–‰
    if ensemble_strategy in ["hard_voting", "soft_voting", "length_based", "realtime_token", "logit_level"]:
        return run_single_method(ensemble_strategy)
    
    # ê¸°ì¡´ ë‹¨ì¼ ì „ëµ ì‹¤í–‰ (í•˜ìœ„ í˜¸í™˜ì„±)
    log.info(f"ì„ íƒëœ ì•™ìƒë¸” ì „ëµ: {ensemble_strategy}")
    
    # ê³µí†µ í•¨ìˆ˜ë¡œ ëª¨ë¸ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    existing_model_paths = get_model_paths()
    if not existing_model_paths:
        return
    
    log.info(f"ì´ {len(existing_model_paths)}ê°œ ëª¨ë¸ë¡œ ì•™ìƒë¸” ì§„í–‰")
    
    # ì•™ìƒë¸” ê°ì²´ ìƒì„±
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    if ensemble_strategy == "realtime_token_ensemble":
        ensemble = RealtimeTokenEnsemble(existing_model_paths, device=device)
    else:  # post_token_voting (default)
        ensemble = PostProcessingEnsemble(existing_model_paths, device=device)
    
    # ëª¨ë¸ë“¤ ë¡œë”©
    try:
        ensemble.load_models()
    except Exception as e:
        log.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return
    
    # ê²€ì¦ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰
    val_data_path = "../../input/data/dev.csv"
    evaluation_results = None
    
    if os.path.exists(val_data_path):
        try:
            log.info("="*50)
            log.info("ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
            log.info("="*50)
            evaluation_results = ensemble.evaluate_on_validation(val_data_path)
            
            # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¡œê¹…
            log.info("ê°œë³„ ëª¨ë¸ ì„±ëŠ¥:")
            for score_info in evaluation_results['individual_model_scores']:
                model_idx = score_info['model_index']
                scores = score_info['rouge_scores']
                model_name = score_info['model_metadata'].get('wandb_run_name', f'Model_{model_idx}')
                log.info(f"  {model_name}: ROUGE-avg={scores['rouge-avg']:.4f}")
            
            # ì•™ìƒë¸” ì„±ëŠ¥ ë¡œê¹…
            ensemble_scores = evaluation_results['ensemble_scores']
            log.info(f"ì•™ìƒë¸” ì„±ëŠ¥: ROUGE-avg={ensemble_scores['rouge-avg']:.4f}")
            
            # ê°œì„  ì •ë„ ê³„ì‚°
            best_individual_score = max([s['rouge_scores']['rouge-avg'] for s in evaluation_results['individual_model_scores']])
            improvement = ensemble_scores['rouge-avg'] - best_individual_score
            log.info(f"ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ : {improvement:+.4f}")
            
        except Exception as e:
            log.error(f"ê²€ì¦ ë°ì´í„° í‰ê°€ ì‹¤íŒ¨: {e}")
    else:
        log.warning(f"ê²€ì¦ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ (ê²€ì¦ ì ìˆ˜ ê³„ì‚° ê±´ë„ˆëœ¨): {val_data_path}")
    
    # ì•™ìƒë¸” ì¶”ë¡  ì‹¤í–‰
    test_data_path = "../../input/data/test.csv"
    if not os.path.exists(test_data_path):
        log.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_data_path}")
        return
    
    try:
        ensemble_results, individual_results = ensemble.run_ensemble(test_data_path)
    except Exception as e:
        log.error(f"ì•™ìƒë¸” ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ensemble_results í´ë” ìƒì„±
    results_dir = "./ensemble_results"
    os.makedirs(results_dir, exist_ok=True)
    
    # í•˜ë“œ ë³´íŒ… ê²°ê³¼ ì €ì¥
    hard_voting_path = os.path.join(results_dir, f"ensemble_hard_voting_{timestamp}.csv")
    ensemble_results['hard_voting'].to_csv(hard_voting_path, index=False, encoding='utf-8')
    log.info(f"í•˜ë“œ ë³´íŒ… ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {hard_voting_path}")
    
    # ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼ ì €ì¥
    soft_voting_path = os.path.join(results_dir, f"ensemble_soft_voting_{timestamp}.csv")
    ensemble_results['soft_voting'].to_csv(soft_voting_path, index=False, encoding='utf-8')
    log.info(f"ì†Œí”„íŠ¸ ë³´íŒ… ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {soft_voting_path}")
    
    # ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼ ì €ì¥
    length_based_path = os.path.join(results_dir, f"ensemble_length_based_{timestamp}.csv")
    ensemble_results['length_based'].to_csv(length_based_path, index=False, encoding='utf-8')
    log.info(f"ê¸¸ì´ ê¸°ë°˜ ì•™ìƒë¸” ê²°ê³¼ ì €ì¥: {length_based_path}")
    
    # ê°œë³„ ëª¨ë¸ ê²°ê³¼ë“¤ ì €ì¥
    for i, individual_result in enumerate(individual_results):
        individual_df = pd.DataFrame({
            'fname': ensemble_results['hard_voting']['fname'],  # í•˜ë“œ ë³´íŒ… ê²°ê³¼ì˜ fname ì‚¬ìš©
            'summary': individual_result
        })
        individual_path = os.path.join(results_dir, f"individual_model_{i+1}_{timestamp}.csv")
        individual_df.to_csv(individual_path, index=False, encoding='utf-8')
        log.info(f"ê°œë³„ ëª¨ë¸ {i+1} ê²°ê³¼ ì €ì¥: {individual_path}")
    
    # ì•™ìƒë¸” ë©”íƒ€ë°ì´í„° ì €ì¥
    ensemble_metadata = {
        "timestamp": timestamp,
        "num_models": len(existing_model_paths),
        "model_paths": existing_model_paths,
        "device": device,
        "ensemble_strategies": ["hard_voting", "soft_voting", "length_based"],
        "model_metadata": ensemble.metadata_list,
        "evaluation_results": evaluation_results  # ê²€ì¦ ì ìˆ˜ ê²°ê³¼ ì¶”ê°€
    }
    
    metadata_path = os.path.join(results_dir, f"ensemble_comparison_metadata_{timestamp}.json")
    with open(metadata_path, "w", encoding='utf-8') as f:
        json.dump(ensemble_metadata, f, indent=2, ensure_ascii=False)
    log.info(f"ì•™ìƒë¸” ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    log.info("=" * 50)
    log.info(f"ì•™ìƒë¸” ì¶”ë¡  ì™„ë£Œ! (í•˜ë“œ ë³´íŒ… & ì†Œí”„íŠ¸ ë³´íŒ… & ê¸¸ì´ ê¸°ë°˜)")
    log.info(f"ì‚¬ìš©ëœ ëª¨ë¸ ìˆ˜: {len(existing_model_paths)}")
    log.info(f"í•˜ë“œ ë³´íŒ… ê²°ê³¼: {hard_voting_path}")
    log.info(f"ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼: {soft_voting_path}")
    log.info(f"ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼: {length_based_path}")
    
    # ê²€ì¦ ì ìˆ˜ ìš”ì•½ ì¶œë ¥
    if evaluation_results:
        log.info(f"í‰ê°€ ê²°ê³¼ ìš”ì•½ (í•˜ë“œ vs ì†Œí”„íŠ¸ vs ê¸¸ì´ ê¸°ë°˜ ë¹„êµ):")
        
        # í•˜ë“œ ë³´íŒ… ê²°ê³¼
        hard_scores = evaluation_results['hard_voting_scores']
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-1: {hard_scores['rouge-1']:.4f}")
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-2: {hard_scores['rouge-2']:.4f}")
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-L: {hard_scores['rouge-l']:.4f}")
        log.info(f"  í•˜ë“œ ë³´íŒ… ROUGE-avg: {hard_scores['rouge-avg']:.4f}")
        
        # ì†Œí”„íŠ¸ ë³´íŒ… ê²°ê³¼
        soft_scores = evaluation_results['soft_voting_scores']
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-1: {soft_scores['rouge-1']:.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-2: {soft_scores['rouge-2']:.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-L: {soft_scores['rouge-l']:.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ROUGE-avg: {soft_scores['rouge-avg']:.4f}")
        
        # ê¸¸ì´ ê¸°ë°˜ ê²°ê³¼
        length_scores = evaluation_results['length_based_scores']
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-1: {length_scores['rouge-1']:.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-2: {length_scores['rouge-2']:.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-L: {length_scores['rouge-l']:.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ROUGE-avg: {length_scores['rouge-avg']:.4f}")
        
        # ìµœê³  ì„±ëŠ¥ ë°©ì‹
        best_method = evaluation_results.get('best_ensemble_method', 'Unknown')
        log.info(f"  ìµœê³  ì„±ëŠ¥ ë°©ì‹: {best_method}")
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        best_individual_score = max([s['rouge_scores']['rouge-avg'] for s in evaluation_results['individual_model_scores']])
        hard_improvement = hard_scores['rouge-avg'] - best_individual_score
        soft_improvement = soft_scores['rouge-avg'] - best_individual_score
        length_improvement = length_scores['rouge-avg'] - best_individual_score
        log.info(f"  í•˜ë“œ ë³´íŒ… ê°œì„ : {hard_improvement:+.4f}")
        log.info(f"  ì†Œí”„íŠ¸ ë³´íŒ… ê°œì„ : {soft_improvement:+.4f}")
        log.info(f"  ê¸¸ì´ ê¸°ë°˜ ê°œì„ : {length_improvement:+.4f}")
        
        # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸ ì •ë³´
        log.info("ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ìƒì„¸:")
        for i, score_info in enumerate(evaluation_results['individual_model_scores']):
            scores = score_info['rouge_scores']
            model_name = score_info['model_metadata'].get('wandb_run_name', f'Model_{i+1}')
            log.info(f"    {model_name}: ROUGE-avg={scores['rouge-avg']:.4f}")
    
    log.info("=" * 50)
    
    return evaluation_results

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='ì•™ìƒë¸” ì¶”ë¡  ì‹œìŠ¤í…œ - ì—¬ëŸ¬ ëª¨ë¸ì„ ì•™ìƒë¸”í•˜ì—¬ í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python ensemble_inference.py                     # ëª¨ë“  ë°©ì‹ ë¹„êµ ì‹¤í–‰
  python ensemble_inference.py --mode=all          # ëª¨ë“  ë°©ì‹ ë¹„êµ ì‹¤í–‰  
  python ensemble_inference.py --mode=hard_voting  # í•˜ë“œ ë³´íŒ…ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=soft_voting  # ì†Œí”„íŠ¸ ë³´íŒ…ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=length_based # ê¸¸ì´ ê¸°ë°˜ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=realtime_token # ì‹¤ì‹œê°„ í† í° ì•™ìƒë¸”ë§Œ ì‹¤í–‰
  python ensemble_inference.py --mode=logit_level    # ìµœì í™”ëœ Logit ì•™ìƒë¸”ë§Œ ì‹¤í–‰

ì•™ìƒë¸” ë°©ì‹ ì„¤ëª…:
  all           - ëª¨ë“  ë°©ì‹ì„ ë¹„êµí•˜ì—¬ ìµœì  ë°©ì‹ ì¶”ì²œ
  hard_voting   - ê° ëª¨ë¸ì´ ì™„ì „í•œ í…ìŠ¤íŠ¸ ìƒì„± í›„ í† í°ë³„ ë‹¤ìˆ˜ê²°
  soft_voting   - ê° ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìµœì  í›„ë³´ ì„ íƒ
  length_based  - ê° ëª¨ë¸ ê²°ê³¼ ì¤‘ ê°€ì¥ ê¸´ ê²ƒì„ ì„ íƒ
  realtime_token- ë§¤ í† í°ë§ˆë‹¤ ëª¨ë“  ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ë¥¼ í‰ê· í•˜ì—¬ ìƒì„±
  logit_level   - ìµœì í™”ëœ Logit ì•™ìƒë¸” (Nucleus Sampling + Beam Search)
        """)
    
    parser.add_argument(
        '--mode', 
        type=str, 
        default='all',
        choices=['all', 'hard_voting', 'soft_voting', 'length_based', 'realtime_token', 'logit_level'],
        help='ì‹¤í–‰í•  ì•™ìƒë¸” ë°©ì‹ ì„ íƒ (ê¸°ë³¸ê°’: all - ëª¨ë“  ë°©ì‹ ë¹„êµ)'
    )
    
    args = parser.parse_args()
    
    # ì„ íƒëœ ëª¨ë“œ ë¡œê¹…
    if args.mode == 'all':
        log.info("ğŸ”¬ ëª¨ë“  ì•™ìƒë¸” ë°©ì‹ ë¹„êµ ëª¨ë“œ ì‹œì‘")
        main("comprehensive")
    else:
        log.info(f"ğŸ¯ ê°œë³„ ë°©ì‹ ì‹¤í–‰ ëª¨ë“œ: {args.mode}")
        main(args.mode)